# Learner pathway modeling information for MITxPRO Additive Manufacturing and LaaL courses

To clone the repository:

```
git clone https://github.com/mginda/edx-learnerpathway-modeling.git

```
### Python imports
![imports](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/images/imports.png)

### Load data helper file
The ```data_parsing.py``` does two things.
1. The ```get_trajectory()``` function binds an ascending order number to course material URLs into a list.  The order number is aligned with the order of the course design.  The order number is returned.
2. The ```load_trajectories()``` reads in multiple learner log data files.  A list of lists of trajectories are returned grouped by student user ids.
```python
def get_trajectory(df):
    order = [0]
    for url in df['order']:
        if url != order[-1]:
            order.append(url)
    order.append(0)
    return order

def load_trajectories(folder_path):
	trajectories = []

	for fname in glob(folder_path + '/*.csv'):
	    df = pd.read_csv(fname)
	    trajectories.append(df)
	    
	trajectories = pd.concat(trajectories)
	return trajectories.groupby('user_id').apply(get_trajectory)
```

  
## Model Overview

Long Short Term Memory (LSTM) neural network and K-medoid clustering is used to identify and predict navigational sequences i.e. trajectories derived from clickstream data.  The model seeks to learn how students progress through course design, and how the interplay between trajectory choices impacts the students success in the course. For this project, success and non-success is defined by the student receiving a certificate for the course, or not.  Clickstream data generated by students is used to learn about the degree to which learner transition activity aligns with the instructor/course design, as well as the models empirically observed emergent learner behavior patterns (LBP).

Visited URLs are the discrete set of choices made by learners.  To model this data, it is necessary to transform this into a continuous vector space through an embedding.  For this model we choose to learn the embedding as part of the broader model optimization rather than attempt to predefine it with a skip-gram or heuristic.

LSTM models are one of the most popular variants of Recurrent Neural Networks (RNN) providing an architecture that is able to adequately represent longer sequences without issues of the vanishing gradient problem.   This work uses the standard definitions of LSTM.  

## Data

## Components used in analysis
#### Note - all code snippets are sampled from the Additive Manufacturing for Innovative Design and Production (MITxPRO+AMxB+1T2018; AM)

### Clustering
For the purpose of clustering only, we vectorize the counts of transitions between URLs for each trajectory.  This is equivalent to bi-grams in natural language processing (NLP).  We do not normalize the counts, as their raw magnitudes capture information on how different students go back and forth through course materials.

![countVectorizer](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/images/cluster_CountVectorizer.png)

We then use a k-medoid scheme to cluster these vectors for various values of k.  We use the elbow method on inter-cluster variance to determine k = 4 is the most natural clustering.

![cluster_function](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/images/clustering_function.png)
![sample_variance](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/images/cluster_var_sample.png)

### Baseline Model
For the baseline model we look at sequences of URLs within a given trajectory.  At its simplest level we can make predictions on the most likely next URL based only on the current URL only.  This is a Markov model with no hidden state.

![baselineModel](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/images/baseline_model.png)

This results in 0.49 accuracy.  We also explored using a longer history of URLs to predict the next URL.  Without a smoothing parameter the highest accuracy was achieved with 3 element histories i.e. URL sequence of length 3, which moved accuracy to 0.54.

![baseline_acc](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/images/baseline_acc_calc.png)

### Trajectory LSTM Model
Our model converts a trajectory to an embedded vector representation which is then processed through an LSTM model.  The output of the LSTM goes through a Dense layer with softmax to assign the probabilities of each next possible URL.  

![LSTM model](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/images/LSTM_hiddim30embdim30.png)

### Conditional Trajectory LSTM Model
To expand upon our intial model, we want to understand if the choices of successful and non-successful learners differ.  We process the embedding and LSTM as before.  In the conditional model a second embedding is also learned that provides a vector of weights to multiply against the LSTM output.  This acts as a mask like operation, effectively upweighting and down weighting different parts of information from the possible next URL choice.  The output of this multiplication is then past through a Dense layer with softmax as before.

![CondLSTM](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/images/CondLSTM_hiddim30embdim30.png)

## File details

  [Data Parsing for models](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/data_parsing.py)
  
  [LSTM model](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/learning_pathways.ipynb)
  
  [Baseline model](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/baseline_model.ipynb)
  
  [K-medoid model](https://github.com/mginda/edx-learnerpathway-modeling/blob/python/clustering_trajectories.ipynb)

