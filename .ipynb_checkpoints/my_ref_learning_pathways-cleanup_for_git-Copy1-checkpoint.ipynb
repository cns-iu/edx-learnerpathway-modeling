{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, Reshape, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from glob import glob\n",
    "from data_parsing import load_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = load_trajectories('data/MITxPRO+AMxB+1T2018/edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1121"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete course structure and module descriptions\n",
    "# list of student identifiers and performance statistics, certification, and enrollment data\n",
    "# problem UTF-encoding error\n",
    "\n",
    "AM_modules = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-modules.txt', sep='\\t', encoding='utf-16')\n",
    "len(AM_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session level edge data\n",
    "AM_edges_sessionLevel01 = pd.read_csv('data/MITxPRO+AMxB+1T2018/edges/MITxPRO+AMxB+1T2018-stdAgg-edgeList-sessionLevel-1.csv')\n",
    "#AM_edges_sessionLevel01[AM_edges_sessionLevel01['user_id'] == 15779327][100:150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3483"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # for each student this is list of numerical id's for the URLs - removing duplicates\n",
    "# def get_trajectory(df):\n",
    "#     order = [0]\n",
    "#     for url in df['order']:\n",
    "#         if url != order[-1]:\n",
    "#             order.append(url)\n",
    "#     order.append(0)\n",
    "#     return order\n",
    "\n",
    "# trajectories = []\n",
    "\n",
    "# for fname in glob('data/MITxPRO+AMxB+1T2018/edges/*.csv'):\n",
    "#     df = pd.read_csv(fname)\n",
    "#     trajectories.append(df)\n",
    "    \n",
    "# trajectories = pd.concat(trajectories)\n",
    "# trajectories = trajectories.groupby('user_id').apply(get_trajectory)\n",
    "traj_lengths = trajectories.map(len).values\n",
    "\n",
    "plt.hist(traj_lengths)\n",
    "max(traj_lengths)\n",
    "#trajectories[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_traj = []\n",
    "outgoing_traj = []\n",
    "n_train = 800\n",
    "n_valid = 130\n",
    "\n",
    "for traj in trajectories.values:\n",
    "    incoming_traj.append(np.array(traj[:-1]).reshape(1,-1))\n",
    "    outgoing_traj.append(np.array(traj[1:]).reshape(-1,1))\n",
    "\n",
    "index = np.arange(len(trajectories))\n",
    "np.random.seed(9)\n",
    "np.random.shuffle(index)\n",
    "    \n",
    "def train_generator():\n",
    "    while True:\n",
    "        for i in range(0, n_train):\n",
    "            x = incoming_traj[index[i]].reshape(1,-1)\n",
    "            y = outgoing_traj[index[i]].reshape(1,-1)\n",
    "            yield x,y\n",
    "            \n",
    "def valid_generator():\n",
    "    while True:\n",
    "        for i in range(n_train, n_train+n_valid):\n",
    "            x = incoming_traj[index[i]].reshape(1,-1)\n",
    "            y = outgoing_traj[index[i]].reshape(1,-1)\n",
    "            yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "URL_embedding (Embedding)    (None, None, 30)          33630     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, None, 37)          10064     \n",
      "_________________________________________________________________\n",
      "Predicted_URL (Dense)        (None, None, 1121)        42598     \n",
      "=================================================================\n",
      "Total params: 86,292\n",
      "Trainable params: 86,292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 37\n",
    "embedding_dim = 30\n",
    "number_of_URL = 1121\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "input_ = Input(shape=(None,))\n",
    "embed = Embedding(number_of_URL, embedding_dim, name='URL_embedding')(input_)\n",
    "\n",
    "#return_sequences = true bc we want the whole sequence returned at every step in the series\n",
    "rnn = LSTM(hidden_dim, return_sequences=True, name='LSTM')(embed)\n",
    "\n",
    "predicted_URL = Dense(number_of_URL, activation = 'softmax', name='Predicted_URL')(rnn)\n",
    "\n",
    "model = Model(inputs=input_, outputs=predicted_URL)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_traj = []\n",
    "outgoing_traj = []\n",
    "n_train = 800\n",
    "n_valid = 130\n",
    "\n",
    "for traj in trajectories.values:\n",
    "    incoming_traj.append(np.array(traj[:-1]).reshape(1,-1))\n",
    "    outgoing_traj.append(np.array(traj[1:]).reshape(-1,1))\n",
    "\n",
    "#add data for pass/fail to status\n",
    "AM_id_and_performance = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "status = np.where(AM_id_and_performance['certGrp']=='Certified (< 70% Grade)', 1, 0)\n",
    "#status = np.where(AM_id_and_performance['certGrp']=='Certified (< 70% Grade)')\n",
    "    \n",
    "index = np.arange(len(trajectories))\n",
    "np.random.seed(9)\n",
    "np.random.shuffle(index)\n",
    "    \n",
    "def train_generator2():\n",
    "    while True:\n",
    "        for i in range(0, n_train):\n",
    "            x = incoming_traj[index[i]].reshape(1,-1)\n",
    "            s = np.broadcast_to(status[index[i]], x.shape)\n",
    "            #s = status[index[i]].reshape(1,-1)\n",
    "            y = outgoing_traj[index[i]].reshape(1,-1)\n",
    "            yield [x,s],y\n",
    "            \n",
    "def valid_generator2():\n",
    "    while True:\n",
    "        for i in range(n_train, n_train+n_valid):\n",
    "            x = incoming_traj[index[i]].reshape(1,-1)\n",
    "            s = np.broadcast_to(status[index[i]], x.shape)\n",
    "            #status[index[i]].reshape(1,-1)\n",
    "            y = outgoing_traj[index[i]].reshape(1,-1)\n",
    "            yield [x,s],y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "URL_embedding (Embedding)       (None, None, 30)     33630       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM (LSTM)                     (None, None, 37)     10064       URL_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Status_embedding (Embedding)    (None, None, 37)     74          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, None, 37)     0           LSTM[0][0]                       \n",
      "                                                                 Status_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Predicted_URL (Dense)           (None, None, 1121)   42598       multiply[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 86,366\n",
      "Trainable params: 86,366\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 37\n",
    "embedding_dim = 30\n",
    "number_of_URL = 1121\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "input_x = Input(shape=(None,))\n",
    "input_s = Input(shape=(None,))\n",
    "embed_x = Embedding(number_of_URL, embedding_dim, name='URL_embedding')(input_x)\n",
    "embed_s = Embedding(2, hidden_dim, embeddings_initializer='ones', name='Status_embedding')(input_s)\n",
    "\n",
    "rnn = LSTM(hidden_dim, return_sequences=True, name='LSTM')(embed_x)\n",
    "masked = Multiply()([rnn, embed_s])\n",
    "\n",
    "predicted_URL = Dense(number_of_URL, activation = 'softmax', name='Predicted_URL')(masked)\n",
    "\n",
    "model2 = Model(inputs=[input_x, input_s], outputs=predicted_URL)\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('weights_version01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 800 steps, validate for 130 steps\n",
      "Epoch 1/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 3.6170 - acc: 0.1705\n",
      "Epoch 00001: val_loss improved from inf to 3.03037, saving model to weights-improvement-01-3.03.hdf5\n",
      "800/800 [==============================] - 523s 653ms/step - loss: 3.6168 - acc: 0.1706 - val_loss: 3.0304 - val_acc: 0.2584\n",
      "Epoch 2/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 2.6146 - acc: 0.3666\n",
      "Epoch 00002: val_loss improved from 3.03037 to 2.40647, saving model to weights-improvement-02-2.41.hdf5\n",
      "800/800 [==============================] - 519s 648ms/step - loss: 2.6149 - acc: 0.3667 - val_loss: 2.4065 - val_acc: 0.4466\n",
      "Epoch 3/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 2.1617 - acc: 0.4973\n",
      "Epoch 00003: val_loss improved from 2.40647 to 2.14224, saving model to weights-improvement-03-2.14.hdf5\n",
      "800/800 [==============================] - 654s 817ms/step - loss: 2.1622 - acc: 0.4972 - val_loss: 2.1422 - val_acc: 0.5143\n",
      "Epoch 4/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.9674 - acc: 0.5378\n",
      "Epoch 00004: val_loss improved from 2.14224 to 2.02634, saving model to weights-improvement-04-2.03.hdf5\n",
      "800/800 [==============================] - 603s 754ms/step - loss: 1.9680 - acc: 0.5377 - val_loss: 2.0263 - val_acc: 0.5340\n",
      "Epoch 5/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.8702 - acc: 0.5541\n",
      "Epoch 00005: val_loss improved from 2.02634 to 1.96100, saving model to weights-improvement-05-1.96.hdf5\n",
      "800/800 [==============================] - 602s 753ms/step - loss: 1.8708 - acc: 0.5540 - val_loss: 1.9610 - val_acc: 0.5462\n",
      "Epoch 6/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.8120 - acc: 0.5642\n",
      "Epoch 00006: val_loss improved from 1.96100 to 1.91921, saving model to weights-improvement-06-1.92.hdf5\n",
      "800/800 [==============================] - 422s 528ms/step - loss: 1.8126 - acc: 0.5641 - val_loss: 1.9192 - val_acc: 0.5533\n",
      "Epoch 7/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.7730 - acc: 0.5704\n",
      "Epoch 00007: val_loss improved from 1.91921 to 1.88994, saving model to weights-improvement-07-1.89.hdf5\n",
      "800/800 [==============================] - 360s 450ms/step - loss: 1.7736 - acc: 0.5703 - val_loss: 1.8899 - val_acc: 0.5576\n",
      "Epoch 8/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.7452 - acc: 0.5751\n",
      "Epoch 00008: val_loss improved from 1.88994 to 1.86853, saving model to weights-improvement-08-1.87.hdf5\n",
      "800/800 [==============================] - 356s 445ms/step - loss: 1.7458 - acc: 0.5750 - val_loss: 1.8685 - val_acc: 0.5610\n",
      "Epoch 9/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.7230 - acc: 0.5790\n",
      "Epoch 00009: val_loss improved from 1.86853 to 1.85165, saving model to weights-improvement-09-1.85.hdf5\n",
      "800/800 [==============================] - 361s 452ms/step - loss: 1.7237 - acc: 0.5789 - val_loss: 1.8516 - val_acc: 0.5648\n",
      "Epoch 10/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.7052 - acc: 0.5823\n",
      "Epoch 00010: val_loss improved from 1.85165 to 1.83798, saving model to weights-improvement-10-1.84.hdf5\n",
      "800/800 [==============================] - 307s 384ms/step - loss: 1.7058 - acc: 0.5822 - val_loss: 1.8380 - val_acc: 0.5686\n",
      "Epoch 11/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6904 - acc: 0.5854\n",
      "Epoch 00011: val_loss improved from 1.83798 to 1.82709, saving model to weights-improvement-11-1.83.hdf5\n",
      "800/800 [==============================] - 268s 335ms/step - loss: 1.6911 - acc: 0.5853 - val_loss: 1.8271 - val_acc: 0.5718\n",
      "Epoch 12/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6776 - acc: 0.5882\n",
      "Epoch 00012: val_loss improved from 1.82709 to 1.81764, saving model to weights-improvement-12-1.82.hdf5\n",
      "800/800 [==============================] - 269s 336ms/step - loss: 1.6783 - acc: 0.5881 - val_loss: 1.8176 - val_acc: 0.5746\n",
      "Epoch 13/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6665 - acc: 0.5904\n",
      "Epoch 00013: val_loss improved from 1.81764 to 1.80955, saving model to weights-improvement-13-1.81.hdf5\n",
      "800/800 [==============================] - 269s 336ms/step - loss: 1.6672 - acc: 0.5903 - val_loss: 1.8095 - val_acc: 0.5762\n",
      "Epoch 14/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6566 - acc: 0.5923\n",
      "Epoch 00014: val_loss improved from 1.80955 to 1.80152, saving model to weights-improvement-14-1.80.hdf5\n",
      "800/800 [==============================] - 269s 336ms/step - loss: 1.6572 - acc: 0.5922 - val_loss: 1.8015 - val_acc: 0.5775\n",
      "Epoch 15/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6479 - acc: 0.5940\n",
      "Epoch 00015: val_loss improved from 1.80152 to 1.79435, saving model to weights-improvement-15-1.79.hdf5\n",
      "800/800 [==============================] - 269s 337ms/step - loss: 1.6485 - acc: 0.5939 - val_loss: 1.7943 - val_acc: 0.5794\n",
      "Epoch 16/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6401 - acc: 0.5953\n",
      "Epoch 00016: val_loss improved from 1.79435 to 1.78877, saving model to weights-improvement-16-1.79.hdf5\n",
      "800/800 [==============================] - 271s 339ms/step - loss: 1.6407 - acc: 0.5952 - val_loss: 1.7888 - val_acc: 0.5808\n",
      "Epoch 17/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6354 - acc: 0.5962\n",
      "Epoch 00017: val_loss improved from 1.78877 to 1.78493, saving model to weights-improvement-17-1.78.hdf5\n",
      "800/800 [==============================] - 269s 336ms/step - loss: 1.6361 - acc: 0.5961 - val_loss: 1.7849 - val_acc: 0.5813\n",
      "Epoch 18/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6270 - acc: 0.5976\n",
      "Epoch 00018: val_loss improved from 1.78493 to 1.77993, saving model to weights-improvement-18-1.78.hdf5\n",
      "800/800 [==============================] - 269s 337ms/step - loss: 1.6276 - acc: 0.5975 - val_loss: 1.7799 - val_acc: 0.5827\n",
      "Epoch 19/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6209 - acc: 0.5987\n",
      "Epoch 00019: val_loss improved from 1.77993 to 1.77594, saving model to weights-improvement-19-1.78.hdf5\n",
      "800/800 [==============================] - 285s 356ms/step - loss: 1.6216 - acc: 0.5986 - val_loss: 1.7759 - val_acc: 0.5833\n",
      "Epoch 20/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6153 - acc: 0.5997\n",
      "Epoch 00020: val_loss improved from 1.77594 to 1.77227, saving model to weights-improvement-20-1.77.hdf5\n",
      "800/800 [==============================] - 271s 338ms/step - loss: 1.6160 - acc: 0.5996 - val_loss: 1.7723 - val_acc: 0.5845\n",
      "Epoch 21/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6102 - acc: 0.6006\n",
      "Epoch 00021: val_loss improved from 1.77227 to 1.76905, saving model to weights-improvement-21-1.77.hdf5\n",
      "800/800 [==============================] - 266s 332ms/step - loss: 1.6109 - acc: 0.6005 - val_loss: 1.7691 - val_acc: 0.5851\n",
      "Epoch 22/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6058 - acc: 0.6015\n",
      "Epoch 00022: val_loss improved from 1.76905 to 1.76622, saving model to weights-improvement-22-1.77.hdf5\n",
      "800/800 [==============================] - 266s 332ms/step - loss: 1.6064 - acc: 0.6014 - val_loss: 1.7662 - val_acc: 0.5857\n",
      "Epoch 23/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.6013 - acc: 0.6025\n",
      "Epoch 00023: val_loss improved from 1.76622 to 1.76371, saving model to weights-improvement-23-1.76.hdf5\n",
      "800/800 [==============================] - 265s 332ms/step - loss: 1.6019 - acc: 0.6024 - val_loss: 1.7637 - val_acc: 0.5865\n",
      "Epoch 24/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5973 - acc: 0.6031\n",
      "Epoch 00024: val_loss improved from 1.76371 to 1.76082, saving model to weights-improvement-24-1.76.hdf5\n",
      "800/800 [==============================] - 269s 336ms/step - loss: 1.5980 - acc: 0.6030 - val_loss: 1.7608 - val_acc: 0.5874\n",
      "Epoch 25/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5933 - acc: 0.6039\n",
      "Epoch 00025: val_loss improved from 1.76082 to 1.75811, saving model to weights-improvement-25-1.76.hdf5\n",
      "800/800 [==============================] - 279s 349ms/step - loss: 1.5940 - acc: 0.6038 - val_loss: 1.7581 - val_acc: 0.5883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5897 - acc: 0.6046\n",
      "Epoch 00026: val_loss improved from 1.75811 to 1.75567, saving model to weights-improvement-26-1.76.hdf5\n",
      "800/800 [==============================] - 288s 360ms/step - loss: 1.5903 - acc: 0.6045 - val_loss: 1.7557 - val_acc: 0.5886\n",
      "Epoch 27/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5863 - acc: 0.6054\n",
      "Epoch 00027: val_loss improved from 1.75567 to 1.75400, saving model to weights-improvement-27-1.75.hdf5\n",
      "800/800 [==============================] - 274s 342ms/step - loss: 1.5869 - acc: 0.6053 - val_loss: 1.7540 - val_acc: 0.5890\n",
      "Epoch 28/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5830 - acc: 0.6060\n",
      "Epoch 00028: val_loss improved from 1.75400 to 1.75195, saving model to weights-improvement-28-1.75.hdf5\n",
      "800/800 [==============================] - 278s 347ms/step - loss: 1.5836 - acc: 0.6059 - val_loss: 1.7520 - val_acc: 0.5895\n",
      "Epoch 29/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5797 - acc: 0.6066\n",
      "Epoch 00029: val_loss improved from 1.75195 to 1.74985, saving model to weights-improvement-29-1.75.hdf5\n",
      "800/800 [==============================] - 294s 367ms/step - loss: 1.5804 - acc: 0.6065 - val_loss: 1.7499 - val_acc: 0.5902\n",
      "Epoch 30/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5767 - acc: 0.6071\n",
      "Epoch 00030: val_loss improved from 1.74985 to 1.74833, saving model to weights-improvement-30-1.75.hdf5\n",
      "800/800 [==============================] - 309s 386ms/step - loss: 1.5774 - acc: 0.6070 - val_loss: 1.7483 - val_acc: 0.5905\n",
      "Epoch 31/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5743 - acc: 0.6076\n",
      "Epoch 00031: val_loss improved from 1.74833 to 1.74696, saving model to weights-improvement-31-1.75.hdf5\n",
      "800/800 [==============================] - 318s 398ms/step - loss: 1.5750 - acc: 0.6075 - val_loss: 1.7470 - val_acc: 0.5907\n",
      "Epoch 32/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5712 - acc: 0.6081\n",
      "Epoch 00032: val_loss improved from 1.74696 to 1.74526, saving model to weights-improvement-32-1.75.hdf5\n",
      "800/800 [==============================] - 296s 370ms/step - loss: 1.5719 - acc: 0.6080 - val_loss: 1.7453 - val_acc: 0.5910\n",
      "Epoch 33/150\n",
      "799/800 [============================>.] - ETA: 0s - loss: 1.5685 - acc: 0.6085\n",
      "Epoch 00033: val_loss improved from 1.74526 to 1.74359, saving model to weights-improvement-33-1.74.hdf5\n",
      "800/800 [==============================] - 294s 368ms/step - loss: 1.5692 - acc: 0.6084 - val_loss: 1.7436 - val_acc: 0.5911\n",
      "Epoch 34/150\n",
      "211/800 [======>.......................] - ETA: 3:24 - loss: 1.5418 - acc: 0.6171"
     ]
    }
   ],
   "source": [
    "#model.load_weights('weights-improvement-01-4.50.hdf5', by_name=True)\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit_generator(train_generator(), \n",
    "                    validation_data=valid_generator(),\n",
    "                    callbacks=callbacks_list,\n",
    "                    steps_per_epoch = n_train, #batch size is inherently 1 via generator\n",
    "                    validation_steps= n_valid,\n",
    "                    epochs=150,\n",
    "                    verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-userList-key-sessionLevel.csv' does not exist: b'edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-userList-key-sessionLevel.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a4de10f14eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# user list key - session level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mAM_userList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-userList-key-sessionLevel.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mAM_userList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# learning pathway network edge lists - edge list for each student in the course that represent a directed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-userList-key-sessionLevel.csv' does not exist: b'edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-userList-key-sessionLevel.csv'"
     ]
    }
   ],
   "source": [
    "# # user list key - session level\n",
    "# AM_userList = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-userList-key-sessionLevel.csv')\n",
    "# AM_userList\n",
    "\n",
    "# # learning pathway network edge lists - edge list for each student in the course that represent a directed \n",
    "# # transitions networks  of students pathway through the courses content modules.  this is all students.\n",
    "# AM_edgelist = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-edges-cohort.csv')\n",
    "# AM_edgelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 492)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incoming_traj[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0008917920058593154,\n",
       " 0.0008922498673200607,\n",
       " 0.0008929030736908317,\n",
       " 0.000891946954652667,\n",
       " 0.0008934624493122101,\n",
       " 0.0008928512106649578,\n",
       " 0.0008921678527258337,\n",
       " 0.000891458592377603,\n",
       " 0.0008932806085795164,\n",
       " 0.0008928888128139079,\n",
       " 0.0008910036995075643,\n",
       " 0.0008931730408221483,\n",
       " 0.0008927834569476545,\n",
       " 0.0008917568484321237,\n",
       " 0.0008912365301512182,\n",
       " 0.0008933590725064278,\n",
       " 0.0008923557470552623,\n",
       " 0.0008924041176214814,\n",
       " 0.0008910631877370179,\n",
       " 0.0008918875828385353,\n",
       " 0.0008920537657104433,\n",
       " 0.0008927110466174781,\n",
       " 0.0008914669160731137,\n",
       " 0.0008930089534260333,\n",
       " 0.0008908484014682472,\n",
       " 0.0008929602918215096,\n",
       " 0.0008919625543057919,\n",
       " 0.0008919555111788213,\n",
       " 0.0008924801368266344,\n",
       " 0.0008918195380829275,\n",
       " 0.0008930262411013246,\n",
       " 0.0008931169286370277,\n",
       " 0.0008931318880058825,\n",
       " 0.0008933317149057984,\n",
       " 0.0008945022127591074,\n",
       " 0.0008925190195441246,\n",
       " 0.0008940619300119579,\n",
       " 0.0008919631945900619,\n",
       " 0.0008915046928450465,\n",
       " 0.0008911574259400368,\n",
       " 0.0008912040502764285,\n",
       " 0.0008904650458134711,\n",
       " 0.0008915925864130259,\n",
       " 0.0008906847797334194,\n",
       " 0.0008944850414991379,\n",
       " 0.0008927815360948443,\n",
       " 0.0008921113912947476,\n",
       " 0.0008920588879846036,\n",
       " 0.0008915084763430059,\n",
       " 0.0008926669834181666,\n",
       " 0.0008923257701098919,\n",
       " 0.0008924946887418628,\n",
       " 0.0008920894470065832,\n",
       " 0.0008922736160457134,\n",
       " 0.0008915307116694748,\n",
       " 0.000891899922862649,\n",
       " 0.0008923650602810085,\n",
       " 0.0008908928139135242,\n",
       " 0.0008924352005124092,\n",
       " 0.0008914429345168173,\n",
       " 0.0008931334014050663,\n",
       " 0.000891540723387152,\n",
       " 0.0008905414142645895,\n",
       " 0.0008907123701646924,\n",
       " 0.0008933352655731142,\n",
       " 0.000891413539648056,\n",
       " 0.000891449919436127,\n",
       " 0.0008922654087655246,\n",
       " 0.0008920708787627518,\n",
       " 0.0008923513814806938,\n",
       " 0.0008939487743191421,\n",
       " 0.0008915142389014363,\n",
       " 0.0008928121533244848,\n",
       " 0.0008938319515436888,\n",
       " 0.0008932290365919471,\n",
       " 0.0008921170374378562,\n",
       " 0.0008913840865716338,\n",
       " 0.000892738695256412,\n",
       " 0.0008944508153945208,\n",
       " 0.0008923600544221699,\n",
       " 0.0008916659280657768,\n",
       " 0.0008922639535740018,\n",
       " 0.0008908827439881861,\n",
       " 0.0008915464859455824,\n",
       " 0.0008925686706788838,\n",
       " 0.0008918026578612626,\n",
       " 0.0008908240124583244,\n",
       " 0.0008919229148887098,\n",
       " 0.0008928963798098266,\n",
       " 0.0008924236171878874,\n",
       " 0.0008912679040804505,\n",
       " 0.0008917085942812264,\n",
       " 0.0008895900682546198,\n",
       " 0.0008943844586610794,\n",
       " 0.0008922992856241763,\n",
       " 0.0008899912354536355,\n",
       " 0.000892008189111948,\n",
       " 0.0008936171070672572,\n",
       " 0.0008933584904298186,\n",
       " 0.0008935723453760147,\n",
       " 0.00089082564227283,\n",
       " 0.0008934552315622568,\n",
       " 0.0008918905514292419,\n",
       " 0.0008914044592529535,\n",
       " 0.0008913486381061375,\n",
       " 0.0008942388230934739,\n",
       " 0.0008918209350667894,\n",
       " 0.0008922347333282232,\n",
       " 0.0008924099383875728,\n",
       " 0.0008920901454985142,\n",
       " 0.0008930355543270707,\n",
       " 0.0008934921934269369,\n",
       " 0.0008931702468544245,\n",
       " 0.0008910168544389307,\n",
       " 0.000894201744813472,\n",
       " 0.0008934392826631665,\n",
       " 0.000892103009391576,\n",
       " 0.0008929705363698304,\n",
       " 0.0008916219812817872,\n",
       " 0.0008922432898543775,\n",
       " 0.0008902795379981399,\n",
       " 0.0008920931722968817,\n",
       " 0.0008910875767469406,\n",
       " 0.000891936884727329,\n",
       " 0.000893782009370625,\n",
       " 0.0008921023108996451,\n",
       " 0.0008922044653445482,\n",
       " 0.0008901343680918217,\n",
       " 0.0008923485293053091,\n",
       " 0.0008919388637878001,\n",
       " 0.0008911775657907128,\n",
       " 0.0008904641726985574,\n",
       " 0.0008927542949095368,\n",
       " 0.0008942088461481035,\n",
       " 0.0008930686162784696,\n",
       " 0.0008931498741731048,\n",
       " 0.000891852134373039,\n",
       " 0.0008933815988712013,\n",
       " 0.0008934202487580478,\n",
       " 0.0008916056249290705,\n",
       " 0.0008916542283259332,\n",
       " 0.0008934992365539074,\n",
       " 0.0008927388116717339,\n",
       " 0.0008915012585930526,\n",
       " 0.0008922166307456791,\n",
       " 0.0008935619262047112,\n",
       " 0.0008917644736357033,\n",
       " 0.0008901766850613058,\n",
       " 0.0008918111561797559,\n",
       " 0.0008909839671105146,\n",
       " 0.0008932598284445703,\n",
       " 0.0008917833911255002,\n",
       " 0.0008921109256334603,\n",
       " 0.0008913726778700948,\n",
       " 0.0008914456120692194,\n",
       " 0.0008932051132433116,\n",
       " 0.0008914191275835037,\n",
       " 0.0008919528918340802,\n",
       " 0.0008917130180634558,\n",
       " 0.0008920311811380088,\n",
       " 0.0008916816441342235,\n",
       " 0.0008931964403018355,\n",
       " 0.0008911378681659698,\n",
       " 0.0008915840880945325,\n",
       " 0.0008912656339816749,\n",
       " 0.0008933786302804947,\n",
       " 0.0008901436813175678,\n",
       " 0.000892423908226192,\n",
       " 0.0008896333165466785,\n",
       " 0.0008908928139135242,\n",
       " 0.0008926563896238804,\n",
       " 0.0008917441591620445,\n",
       " 0.0008919806568883359,\n",
       " 0.0008911375189200044,\n",
       " 0.0008926035952754319,\n",
       " 0.0008909899624995887,\n",
       " 0.0008909283787943423,\n",
       " 0.0008923515561036766,\n",
       " 0.0008920794352889061,\n",
       " 0.0008913291385397315,\n",
       " 0.0008924048743210733,\n",
       " 0.0008910348988138139,\n",
       " 0.0008914819336496294,\n",
       " 0.0008922403794713318,\n",
       " 0.0008924598223529756,\n",
       " 0.0008922496344894171,\n",
       " 0.0008914331556297839,\n",
       " 0.0008915350772440434,\n",
       " 0.000892097654286772,\n",
       " 0.0008910697652027011,\n",
       " 0.0008907305891625583,\n",
       " 0.000891954405233264,\n",
       " 0.0008935850346460938,\n",
       " 0.0008905771537683904,\n",
       " 0.0008912812918424606,\n",
       " 0.0008912305347621441,\n",
       " 0.0008921397384256124,\n",
       " 0.0008931556949391961,\n",
       " 0.0008926222799345851,\n",
       " 0.0008902500849217176,\n",
       " 0.0008919183746911585,\n",
       " 0.0008905809372663498,\n",
       " 0.0008921519038267434,\n",
       " 0.000890972965862602,\n",
       " 0.0008911449695006013,\n",
       " 0.0008924679714255035,\n",
       " 0.0008910064934752882,\n",
       " 0.0008937395759858191,\n",
       " 0.0008907559094950557,\n",
       " 0.0008908051531761885,\n",
       " 0.0008907055016607046,\n",
       " 0.0008930606418289244,\n",
       " 0.0008914403151720762,\n",
       " 0.000893408025149256,\n",
       " 0.0008913386263884604,\n",
       " 0.0008927418966777623,\n",
       " 0.000892403710167855,\n",
       " 0.0008899025851860642,\n",
       " 0.0008920692489482462,\n",
       " 0.0008916294318623841,\n",
       " 0.0008899482199922204,\n",
       " 0.0008896640501916409,\n",
       " 0.0008892853511497378,\n",
       " 0.0008917049854062498,\n",
       " 0.0008914053323678672,\n",
       " 0.0008921053376980126,\n",
       " 0.0008915825164876878,\n",
       " 0.0008913679630495608,\n",
       " 0.0008937461534515023,\n",
       " 0.0008907986339181662,\n",
       " 0.0008931396878324449,\n",
       " 0.0008918957901187241,\n",
       " 0.0008927943417802453,\n",
       " 0.0008921554544940591,\n",
       " 0.000892636482603848,\n",
       " 0.0008918845560401678,\n",
       " 0.0008900963002815843,\n",
       " 0.0008908509626053274,\n",
       " 0.0008915507351048291,\n",
       " 0.0008912120829336345,\n",
       " 0.0008914185455068946,\n",
       " 0.0008925027213990688,\n",
       " 0.0008908064337447286,\n",
       " 0.0008917252998799086,\n",
       " 0.0008922700071707368,\n",
       " 0.0008913492783904076,\n",
       " 0.0008908695890568197,\n",
       " 0.0008908769232220948,\n",
       " 0.0008910659817047417,\n",
       " 0.0008905525901354849,\n",
       " 0.0008929676841944456,\n",
       " 0.0008909776224754751,\n",
       " 0.0008919551037251949,\n",
       " 0.0008924784488044679,\n",
       " 0.0008912135381251574,\n",
       " 0.0008917993982322514,\n",
       " 0.0008919393876567483,\n",
       " 0.0008937789825722575,\n",
       " 0.0008920396212488413,\n",
       " 0.0008916440419852734,\n",
       " 0.0008908073650673032,\n",
       " 0.000892135314643383,\n",
       " 0.0008912552148103714,\n",
       " 0.0008924229769036174,\n",
       " 0.0008932598866522312,\n",
       " 0.0008921351982280612,\n",
       " 0.0008918220992200077,\n",
       " 0.0008920961990952492,\n",
       " 0.0008935127989389002,\n",
       " 0.0008926772279664874,\n",
       " 0.0008887443109415472,\n",
       " 0.0008922596462070942,\n",
       " 0.0008922554552555084,\n",
       " 0.0008913508499972522,\n",
       " 0.00089261558605358,\n",
       " 0.0008925748988986015,\n",
       " 0.0008928778115659952,\n",
       " 0.0008907017763704062,\n",
       " 0.0008919581887312233,\n",
       " 0.0008907331502996385,\n",
       " 0.000891690666321665,\n",
       " 0.0008901933906599879,\n",
       " 0.0008919652900658548,\n",
       " 0.0008915899670682847,\n",
       " 0.0008916594670154154,\n",
       " 0.0008919334504753351,\n",
       " 0.0008913239580579102,\n",
       " 0.0008920382824726403,\n",
       " 0.0008911528275348246,\n",
       " 0.0008940950501710176,\n",
       " 0.00089080142788589,\n",
       " 0.00089312280761078,\n",
       " 0.0008936701924540102,\n",
       " 0.0008929245523177087,\n",
       " 0.0008914945065043867,\n",
       " 0.0008932890486903489,\n",
       " 0.0008916417136788368,\n",
       " 0.0008926750742830336,\n",
       " 0.0008913910714909434,\n",
       " 0.00089028209913522,\n",
       " 0.0008929785108193755,\n",
       " 0.0008918980020098388,\n",
       " 0.00089176872279495,\n",
       " 0.000891725649125874,\n",
       " 0.0008928939350880682,\n",
       " 0.000892758893314749,\n",
       " 0.0008935112855397165,\n",
       " 0.0008921170374378562,\n",
       " 0.0008933792123571038,\n",
       " 0.0008937787497416139,\n",
       " 0.0008905201102606952,\n",
       " 0.0008934959769248962,\n",
       " 0.000892136013135314,\n",
       " 0.0008917350205592811,\n",
       " 0.0008937088423408568,\n",
       " 0.0008908091112971306,\n",
       " 0.0008933931239880621,\n",
       " 0.0008932235650718212,\n",
       " 0.0008928963798098266,\n",
       " 0.0008931143674999475,\n",
       " 0.000891420291736722,\n",
       " 0.0008915721555240452,\n",
       " 0.0008907196461223066,\n",
       " 0.0008923622663132846,\n",
       " 0.0008931999327614903,\n",
       " 0.0008894705097191036,\n",
       " 0.0008934201905503869,\n",
       " 0.0008929439936764538,\n",
       " 0.0008904596325010061,\n",
       " 0.0008928249590098858,\n",
       " 0.0008919166284613311,\n",
       " 0.0008937933016568422,\n",
       " 0.0008934731595218182,\n",
       " 0.0008914788486436009,\n",
       " 0.00089139404008165,\n",
       " 0.0008915526559576392,\n",
       " 0.0008926901500672102,\n",
       " 0.0008906686562113464,\n",
       " 0.0008918805397115648,\n",
       " 0.0008930099429562688,\n",
       " 0.0008913659257814288,\n",
       " 0.0008915258222259581,\n",
       " 0.0008891162578947842,\n",
       " 0.0008915691869333386,\n",
       " 0.0008911837940104306,\n",
       " 0.0008919136016629636,\n",
       " 0.0008937595412135124,\n",
       " 0.000892401032615453,\n",
       " 0.0008913763449527323,\n",
       " 0.0008920872933231294,\n",
       " 0.0008919857209548354,\n",
       " 0.0008919700630940497,\n",
       " 0.0008932873024605215,\n",
       " 0.0008913495694287121,\n",
       " 0.0008927793824113905,\n",
       " 0.0008915872895158827,\n",
       " 0.0008928683237172663,\n",
       " 0.0008912357734516263,\n",
       " 0.0008913516066968441,\n",
       " 0.000891361094545573,\n",
       " 0.0008921088883653283,\n",
       " 0.000890925177372992,\n",
       " 0.0008919397951103747,\n",
       " 0.0008907080045901239,\n",
       " 0.0008932544733397663,\n",
       " 0.0008929348550736904,\n",
       " 0.0008925099391490221,\n",
       " 0.00088999088620767,\n",
       " 0.0008927095332182944,\n",
       " 0.0008917984087020159,\n",
       " 0.0008937203092500567,\n",
       " 0.0008926878217607737,\n",
       " 0.0008936785161495209,\n",
       " 0.0008922626730054617,\n",
       " 0.000891612668056041,\n",
       " 0.0008920217514969409,\n",
       " 0.0008902017725631595,\n",
       " 0.0008910064934752882,\n",
       " 0.0008927391609176993,\n",
       " 0.000892497249878943,\n",
       " 0.0008899313979782164,\n",
       " 0.0008919749525375664,\n",
       " 0.0008908926392905414,\n",
       " 0.0008910667966119945,\n",
       " 0.0008917031227611005,\n",
       " 0.0008911645854823291,\n",
       " 0.0008928931783884764,\n",
       " 0.0008911326294764876,\n",
       " 0.0008916653459891677,\n",
       " 0.000891833973582834,\n",
       " 0.0008915528887882829,\n",
       " 0.0008935810765251517,\n",
       " 0.0008919532992877066,\n",
       " 0.0008915919461287558,\n",
       " 0.0008912496268749237,\n",
       " 0.0008941376581788063,\n",
       " 0.0008932458003982902,\n",
       " 0.0008927854942157865,\n",
       " 0.0008919659303501248,\n",
       " 0.0008922374690882862,\n",
       " 0.0008914884529076517,\n",
       " 0.0008899088716134429,\n",
       " 0.000892205978743732,\n",
       " 0.0008923406130634248,\n",
       " 0.0008921756525523961,\n",
       " 0.0008921971311792731,\n",
       " 0.0008910241303965449,\n",
       " 0.0008935244404710829,\n",
       " 0.0008930520270951092,\n",
       " 0.0008922984125092626,\n",
       " 0.0008923758869059384,\n",
       " 0.0008924120338633657,\n",
       " 0.0008896011277101934,\n",
       " 0.0008898602100089192,\n",
       " 0.000893640797585249,\n",
       " 0.000890561961568892,\n",
       " 0.000892254407517612,\n",
       " 0.0008912725024856627,\n",
       " 0.000890698516741395,\n",
       " 0.0008923772838898003,\n",
       " 0.000892678857780993,\n",
       " 0.0008940365514717996,\n",
       " 0.0008929645409807563,\n",
       " 0.0008927778690122068,\n",
       " 0.0008918996318243444,\n",
       " 0.0008916952647268772,\n",
       " 0.0008915272774174809,\n",
       " 0.0008918201783671975,\n",
       " 0.0008918886305764318,\n",
       " 0.0008915728540159762,\n",
       " 0.0008921594708226621,\n",
       " 0.0008911733166314662,\n",
       " 0.0008914595819078386,\n",
       " 0.0008919363608583808,\n",
       " 0.0008926879963837564,\n",
       " 0.0008896629442460835,\n",
       " 0.0008927721646614373,\n",
       " 0.0008923046407289803,\n",
       " 0.00089135771850124,\n",
       " 0.0008918149396777153,\n",
       " 0.0008914354257285595,\n",
       " 0.0008913353667594492,\n",
       " 0.0008926871814765036,\n",
       " 0.0008929081377573311,\n",
       " 0.0008901054388843477,\n",
       " 0.0008924301946535707,\n",
       " 0.0008929063333198428,\n",
       " 0.0008914639474824071,\n",
       " 0.0008935370133258402,\n",
       " 0.0008900206885300577,\n",
       " 0.0008938585524447262,\n",
       " 0.0008937733364291489,\n",
       " 0.0008906740695238113,\n",
       " 0.0008913527708500624,\n",
       " 0.0008913158671930432,\n",
       " 0.0008912454359233379,\n",
       " 0.0008918476523831487,\n",
       " 0.0008927531307563186,\n",
       " 0.0008927821181714535,\n",
       " 0.0008909861207939684,\n",
       " 0.0008913172059692442,\n",
       " 0.0008925376459956169,\n",
       " 0.0008925406727939844,\n",
       " 0.0008937644306570292,\n",
       " 0.0008923119748942554,\n",
       " 0.000892294105142355,\n",
       " 0.0008939089602790773,\n",
       " 0.0008915934595279396,\n",
       " 0.0008911362965591252,\n",
       " 0.0008927742019295692,\n",
       " 0.0008922078995965421,\n",
       " 0.000893058197107166,\n",
       " 0.0008912949124351144,\n",
       " 0.0008926019654609263,\n",
       " 0.0008891072357073426,\n",
       " 0.0008916886290535331,\n",
       " 0.000892435316927731,\n",
       " 0.0008905553841032088,\n",
       " 0.0008903107373043895,\n",
       " 0.0008926732698455453,\n",
       " 0.0008916272781789303,\n",
       " 0.0008900504326447845,\n",
       " 0.0008932047639973462,\n",
       " 0.0008930388721637428,\n",
       " 0.0008926226873882115,\n",
       " 0.0008945820736698806,\n",
       " 0.0008927676826715469,\n",
       " 0.0008938338723964989,\n",
       " 0.0008923161658458412,\n",
       " 0.000892925017978996,\n",
       " 0.0008901999099180102,\n",
       " 0.000891790259629488,\n",
       " 0.0008953979704529047,\n",
       " 0.0008942073909565806,\n",
       " 0.0008917552186176181,\n",
       " 0.0008923791465349495,\n",
       " 0.0008943506982177496,\n",
       " 0.0008918007952161133,\n",
       " 0.000892380834557116,\n",
       " 0.0008921682019717991,\n",
       " 0.0008931143092922866,\n",
       " 0.0008896428043954074,\n",
       " 0.0008910924661904573,\n",
       " 0.000891628151293844,\n",
       " 0.0008910656906664371,\n",
       " 0.0008928264142014086,\n",
       " 0.0008918552775867283,\n",
       " 0.0008928220486268401,\n",
       " 0.0008925555739551783,\n",
       " 0.0008942438289523125,\n",
       " 0.0008909306488931179,\n",
       " 0.000893537828233093,\n",
       " 0.0008896282524801791,\n",
       " 0.0008924485882744193,\n",
       " 0.0008937639067880809,\n",
       " 0.0008916828082874417,\n",
       " 0.0008926339214667678,\n",
       " 0.0008913169149309397,\n",
       " 0.0008948299800977111,\n",
       " 0.0008910310571081936,\n",
       " 0.0008923592395149171,\n",
       " 0.0008913238998502493,\n",
       " 0.000890580762643367,\n",
       " 0.00089407799532637,\n",
       " 0.0008910396136343479,\n",
       " 0.0008922014385461807,\n",
       " 0.0008912578923627734,\n",
       " 0.0008912535849958658,\n",
       " 0.0008921654080040753,\n",
       " 0.0008925857255235314,\n",
       " 0.0008904440328478813,\n",
       " 0.0008915819344110787,\n",
       " 0.0008921078406274319,\n",
       " 0.00089441635645926,\n",
       " 0.0008904439164325595,\n",
       " 0.0008923435816541314,\n",
       " 0.000892865878995508,\n",
       " 0.000891643576323986,\n",
       " 0.0008933459175750613,\n",
       " 0.0008917803643271327,\n",
       " 0.0008937516831792891,\n",
       " 0.0008931040647439659,\n",
       " 0.0008907150477170944,\n",
       " 0.0008901181281544268,\n",
       " 0.0008920504478737712,\n",
       " 0.0008910718024708331,\n",
       " 0.0008910067263059318,\n",
       " 0.000890723429620266,\n",
       " 0.0008916867082007229,\n",
       " 0.0008910788455978036,\n",
       " 0.0008937345119193196,\n",
       " 0.0008906181901693344,\n",
       " 0.0008922374690882862,\n",
       " 0.0008932447526603937,\n",
       " 0.000890663533937186,\n",
       " 0.0008903240086510777,\n",
       " 0.0008924825233407319,\n",
       " 0.0008920820546336472,\n",
       " 0.0008923007408156991,\n",
       " 0.0008942940039560199,\n",
       " 0.0008907141746021807,\n",
       " 0.0008931529009714723,\n",
       " 0.0008937796810641885,\n",
       " 0.000892037816811353,\n",
       " 0.0008927007438614964,\n",
       " 0.0008923771674744785,\n",
       " 0.0008919364772737026,\n",
       " 0.0008914478239603341,\n",
       " 0.0008912282646633685,\n",
       " 0.000891630130354315,\n",
       " 0.0008942349231801927,\n",
       " 0.0008912481716834009,\n",
       " 0.0008925731526687741,\n",
       " 0.0008903793059289455,\n",
       " 0.0008931568590924144,\n",
       " 0.0008931388729251921,\n",
       " 0.0008917446830309927,\n",
       " 0.0008929471950978041,\n",
       " 0.0008911037584766746,\n",
       " 0.0008920471300370991,\n",
       " 0.0008919095271266997,\n",
       " 0.0008912644116207957,\n",
       " 0.0008916647639125586,\n",
       " 0.0008928554598242044,\n",
       " 0.0008922858978621662,\n",
       " 0.0008925204165279865,\n",
       " 0.0008914175559766591,\n",
       " 0.0008904258138500154,\n",
       " 0.0008925945730879903,\n",
       " 0.0008928027236834168,\n",
       " 0.0008917941595427692,\n",
       " 0.0008910506730899215,\n",
       " 0.0008922859560698271,\n",
       " 0.0008931633783504367,\n",
       " 0.0008906263974495232,\n",
       " 0.000891207018867135,\n",
       " 0.000890865339897573,\n",
       " 0.0008907466544769704,\n",
       " 0.0008912571356631815,\n",
       " 0.0008930936455726624,\n",
       " 0.0008913763449527323,\n",
       " 0.0008916202932596207,\n",
       " 0.0008926462032832205,\n",
       " 0.0008900523534975946,\n",
       " 0.0008935211226344109,\n",
       " 0.0008953568758442998,\n",
       " 0.000891012663487345,\n",
       " 0.0008924271678552032,\n",
       " 0.000894012744538486,\n",
       " 0.0008911564946174622,\n",
       " 0.0008923026616685092,\n",
       " 0.0008921233820728958,\n",
       " 0.000893596385139972,\n",
       " 0.0008931911434046924,\n",
       " 0.0008929292089305818,\n",
       " 0.0008918970124796033,\n",
       " 0.0008955482626333833,\n",
       " 0.0008919308893382549,\n",
       " 0.0008937628590501845,\n",
       " 0.0008940848638303578,\n",
       " 0.0008898884989321232,\n",
       " 0.000891389325261116,\n",
       " 0.0008909369353204966,\n",
       " 0.0008932133787311614,\n",
       " 0.0008933683857321739,\n",
       " 0.000891776115167886,\n",
       " 0.0008923397399485111,\n",
       " 0.0008925916044972837,\n",
       " 0.0008932448108680546,\n",
       " 0.0008903854759410024,\n",
       " 0.0008937639649957418,\n",
       " 0.0008918113890103996,\n",
       " 0.0008911447948776186,\n",
       " 0.00089363066945225,\n",
       " 0.0008924134308472276,\n",
       " 0.0008909822208806872,\n",
       " 0.0008926005102694035,\n",
       " 0.000892122567165643,\n",
       " 0.0008926271111704409,\n",
       " 0.0008919449173845351,\n",
       " 0.000892092299181968,\n",
       " 0.0008938353275880218,\n",
       " 0.0008921397384256124,\n",
       " 0.0008935059304349124,\n",
       " 0.000892228854354471,\n",
       " 0.0008920868276618421,\n",
       " 0.0008909333846531808,\n",
       " 0.0008927955641411245,\n",
       " 0.0008897412335500121,\n",
       " 0.0008930956828407943,\n",
       " 0.0008935668156482279,\n",
       " 0.0008919700630940497,\n",
       " 0.0008921822300180793,\n",
       " 0.0008903347188606858,\n",
       " 0.0008928220486268401,\n",
       " 0.0008917141822166741,\n",
       " 0.0008916525985114276,\n",
       " 0.0008919127285480499,\n",
       " 0.000892112439032644,\n",
       " 0.0008932495256885886,\n",
       " 0.000894351105671376,\n",
       " 0.0008926530135795474,\n",
       " 0.0008946362067945302,\n",
       " 0.0008922869456000626,\n",
       " 0.0008914305362850428,\n",
       " 0.0008934890502132475,\n",
       " 0.0008919111569412053,\n",
       " 0.0008930486510507762,\n",
       " 0.0008924196008592844,\n",
       " 0.0008918145322240889,\n",
       " 0.0008938839891925454,\n",
       " 0.0008929276373237371,\n",
       " 0.0008927833987399936,\n",
       " 0.0008912506164051592,\n",
       " 0.0008907529409043491,\n",
       " 0.0008929976029321551,\n",
       " 0.0008916391525417566,\n",
       " 0.0008918224484659731,\n",
       " 0.0008922730921767652,\n",
       " 0.0008923416608013213,\n",
       " 0.0008929212344810367,\n",
       " 0.0008904716232791543,\n",
       " 0.0008918759413063526,\n",
       " 0.0008914531208574772,\n",
       " 0.0008913881611078978,\n",
       " 0.0008919156389310956,\n",
       " 0.0008920121472328901,\n",
       " 0.0008908170275390148,\n",
       " 0.0008920798427425325,\n",
       " 0.0008921806002035737,\n",
       " 0.0008902190020307899,\n",
       " 0.0008904784917831421,\n",
       " 0.0008935900405049324,\n",
       " 0.0008921013213694096,\n",
       " 0.000890533730853349,\n",
       " 0.0008919501560740173,\n",
       " 0.0008933583740144968,\n",
       " 0.000890828319825232,\n",
       " 0.0008924585999920964,\n",
       " 0.0008915206999517977,\n",
       " 0.000893076416105032,\n",
       " 0.0008919991087168455,\n",
       " 0.0008911179029382765,\n",
       " 0.0008911870536394417,\n",
       " 0.0008911835611797869,\n",
       " 0.0008943508728407323,\n",
       " 0.0008942382992245257,\n",
       " 0.0008938686223700643,\n",
       " 0.0008918411331251264,\n",
       " 0.0008912848425097764,\n",
       " 0.0008916147635318339,\n",
       " 0.0008920165710151196,\n",
       " 0.0008903741254471242,\n",
       " 0.0008927315357141197,\n",
       " 0.0008917766390368342,\n",
       " 0.0008916299557313323,\n",
       " 0.0008942400454543531,\n",
       " 0.0008911082404665649,\n",
       " 0.0008918808889575303,\n",
       " 0.0008909152238629758,\n",
       " 0.0008932107011787593,\n",
       " 0.0008928183815442026,\n",
       " 0.0008925923611968756,\n",
       " 0.0008902602712623775,\n",
       " 0.0008894107886590064,\n",
       " 0.0008932850323617458,\n",
       " 0.0008923534187488258,\n",
       " 0.0008920232066884637,\n",
       " 0.0008911144686862826,\n",
       " 0.000891978619620204,\n",
       " 0.0008920240798033774,\n",
       " 0.0008908748859539628,\n",
       " 0.0008913095225580037,\n",
       " 0.000891763367690146,\n",
       " 0.0008918668609112501,\n",
       " 0.0008924054563976824,\n",
       " 0.0008921105763874948,\n",
       " 0.000892448122613132,\n",
       " 0.0008935382356867194,\n",
       " 0.0008922182023525238,\n",
       " 0.0008930236799642444,\n",
       " 0.0008934558718465269,\n",
       " 0.0008918152307160199,\n",
       " 0.0008921713451854885,\n",
       " 0.0008930189651437104,\n",
       " 0.0008928331080824137,\n",
       " 0.0008930621552281082,\n",
       " 0.0008919138927012682,\n",
       " 0.000890856608748436,\n",
       " 0.0008916717488318682,\n",
       " 0.0008926619193516672,\n",
       " 0.0008935518562793732,\n",
       " 0.0008917342056520283,\n",
       " 0.0008907080045901239,\n",
       " 0.0008936403901316226,\n",
       " 0.0008909838506951928,\n",
       " 0.0008908042218536139,\n",
       " 0.0008904811693355441,\n",
       " 0.0008928402094170451,\n",
       " 0.000892103067599237,\n",
       " 0.0008907929295673966,\n",
       " 0.0008928050519898534,\n",
       " 0.0008921602857299149,\n",
       " 0.0008903636480681598,\n",
       " 0.0008914767531678081,\n",
       " 0.0008905153372325003,\n",
       " 0.0008923286222852767,\n",
       " 0.0008915613871067762,\n",
       " 0.0008908663294278085,\n",
       " 0.0008923276327550411,\n",
       " 0.0008927436429075897,\n",
       " 0.0008924975409172475,\n",
       " 0.0008911265758797526,\n",
       " 0.0008926481823436916,\n",
       " 0.0008907545125111938,\n",
       " 0.0008908804156817496,\n",
       " 0.0008925623842515051,\n",
       " 0.0008916201186366379,\n",
       " 0.0008940332336351275,\n",
       " 0.0008902273839339614,\n",
       " 0.0008939823019318283,\n",
       " 0.0008913062629289925,\n",
       " 0.0008938700775615871,\n",
       " 0.0008919784449972212,\n",
       " 0.0008925547590479255,\n",
       " 0.0008925156434997916,\n",
       " 0.0008922941633500159,\n",
       " 0.0008920683176256716,\n",
       " 0.0008924156427383423,\n",
       " 0.0008918987587094307,\n",
       " 0.0008928249590098858,\n",
       " 0.0008927223389036953,\n",
       " 0.000891385949216783,\n",
       " 0.0008928244933485985,\n",
       " 0.0008930191397666931,\n",
       " 0.0008933358476497233,\n",
       " 0.0008917097002267838,\n",
       " 0.0008916812366805971,\n",
       " 0.0008928528986871243,\n",
       " 0.0008911346085369587,\n",
       " 0.0008922555134631693,\n",
       " 0.0008909562602639198,\n",
       " 0.0008925746660679579,\n",
       " 0.0008908762829378247,\n",
       " 0.0008923538844101131,\n",
       " 0.0008934095385484397,\n",
       " 0.0008926771697588265,\n",
       " 0.0008950104820542037,\n",
       " 0.0008925090078264475,\n",
       " 0.0008925471338443458,\n",
       " 0.0008924691937863827,\n",
       " 0.0008903297130018473,\n",
       " 0.000892968731932342,\n",
       " 0.0008910003816708922,\n",
       " 0.0008907042210921645,\n",
       " 0.0008949571056291461,\n",
       " 0.0008929113973863423,\n",
       " 0.0008904055575840175,\n",
       " 0.0008911211043596268,\n",
       " 0.0008923839777708054,\n",
       " 0.0008926998125389218,\n",
       " 0.0008932037162594497,\n",
       " 0.0008921969565562904,\n",
       " 0.0008928917814046144,\n",
       " 0.0008916017832234502,\n",
       " 0.0008922405540943146,\n",
       " 0.0008923244313336909,\n",
       " 0.0008931275806389749,\n",
       " 0.000892665411811322,\n",
       " 0.0008903382695280015,\n",
       " 0.0008925445727072656,\n",
       " 0.0008922028355300426,\n",
       " 0.0008915591170080006,\n",
       " 0.0008919255342334509,\n",
       " 0.0008913114434108138,\n",
       " 0.0008915934595279396,\n",
       " 0.0008916631923057139,\n",
       " 0.0008928533643484116,\n",
       " 0.0008939638501033187,\n",
       " 0.0008919797837734222,\n",
       " 0.0008920797845348716,\n",
       " 0.0008927073213271797,\n",
       " 0.000892460229806602,\n",
       " 0.0008917386876419187,\n",
       " 0.0008924724534153938,\n",
       " 0.0008933180361054838,\n",
       " 0.0008926421869546175,\n",
       " 0.0008928996394388378,\n",
       " 0.0008926087757572532,\n",
       " 0.0008925870642997324,\n",
       " 0.0008930121548473835,\n",
       " 0.0008896600920706987,\n",
       " 0.0008910605101846159,\n",
       " 0.0008923846180550754,\n",
       " 0.0008920409600250423,\n",
       " 0.0008923591230995953,\n",
       " 0.0008907840237952769,\n",
       " 0.0008928020833991468,\n",
       " 0.0008926282171159983,\n",
       " 0.0008897010702639818,\n",
       " 0.000891601899638772,\n",
       " 0.000892730662599206,\n",
       " 0.0008934212964959443,\n",
       " 0.0008918778039515018,\n",
       " 0.0008920400869101286,\n",
       " 0.0008926242589950562,\n",
       " 0.0008911273325793445,\n",
       " 0.0008912450866773725,\n",
       " 0.0008918234962038696,\n",
       " 0.0008919198298826814,\n",
       " 0.0008917362429201603,\n",
       " 0.0008916233782656491,\n",
       " 0.0008931274642236531,\n",
       " 0.0008933116332627833,\n",
       " 0.000892673444468528,\n",
       " 0.0008917851955629885,\n",
       " 0.0008925053989514709,\n",
       " 0.0008908839081414044,\n",
       " 0.0008936751983128488,\n",
       " 0.0008931802585721016,\n",
       " 0.0008911933400668204,\n",
       " 0.0008905372815206647,\n",
       " 0.00089240912348032,\n",
       " 0.0008916428778320551,\n",
       " 0.0008918923558667302,\n",
       " 0.0008924464927986264,\n",
       " 0.0008911596378311515,\n",
       " 0.0008897801162675023,\n",
       " 0.0008917820523492992,\n",
       " 0.0008919468964450061,\n",
       " 0.0008909545140340924,\n",
       " 0.0008922002743929625,\n",
       " 0.0008918936364352703,\n",
       " 0.0008936020894907415,\n",
       " 0.0008921551052480936,\n",
       " 0.0008912882185541093,\n",
       " 0.0008931015036068857,\n",
       " 0.0008902059053070843,\n",
       " 0.0008911420009098947,\n",
       " 0.0008917785598896444,\n",
       " 0.0008916784427128732,\n",
       " 0.000891649629920721,\n",
       " 0.0008932592463679612,\n",
       " 0.0008924696012400091,\n",
       " 0.000892969430424273,\n",
       " 0.0008922896231524646,\n",
       " 0.0008907467708922923,\n",
       " 0.0008925989386625588,\n",
       " 0.0008927888702601194,\n",
       " 0.0008911498007364571,\n",
       " 0.0008920987602323294,\n",
       " 0.000892871932592243,\n",
       " 0.0008922094129957259,\n",
       " 0.0008926576701924205,\n",
       " 0.0008925197180360556,\n",
       " 0.0008916727965697646,\n",
       " 0.000892902200575918,\n",
       " 0.0008936614613048732,\n",
       " 0.0008907283190637827,\n",
       " 0.0008915031794458628,\n",
       " 0.0008931847405619919,\n",
       " 0.0008930257172323763,\n",
       " 0.0008901407127268612,\n",
       " 0.0008909747120924294,\n",
       " 0.0008938463870435953,\n",
       " 0.0008914638892747462,\n",
       " 0.0008927150629460812,\n",
       " 0.0008931952761486173,\n",
       " 0.0008915602811612189,\n",
       " 0.0008923999848775566,\n",
       " 0.0008921258267946541,\n",
       " 0.0008923106943257153,\n",
       " 0.0008924184367060661,\n",
       " 0.0008904515998438001,\n",
       " 0.0008916652877815068,\n",
       " 0.0008898709202185273,\n",
       " 0.0008939284598454833,\n",
       " 0.0008917260565795004,\n",
       " 0.0008939963881857693,\n",
       " 0.0008936140802688897,\n",
       " 0.0008943705470301211,\n",
       " 0.0008945809095166624,\n",
       " 0.0008925527799874544,\n",
       " 0.0008916030055843294,\n",
       " 0.0008916392689570785,\n",
       " 0.0008930514450185001,\n",
       " 0.0008942064014263451,\n",
       " 0.000893747725058347,\n",
       " 0.0008907600422389805,\n",
       " 0.0008904050919227302,\n",
       " 0.0008932926575653255,\n",
       " 0.0008939413819462061,\n",
       " 0.0008937889942899346,\n",
       " 0.0008932239143177867,\n",
       " 0.0008915028884075582,\n",
       " 0.0008923927671276033,\n",
       " 0.0008914711652323604,\n",
       " 0.0008920964901335537,\n",
       " 0.000890353403519839,\n",
       " 0.0008925741421990097,\n",
       " 0.0008913176716305315,\n",
       " 0.0008913555648177862,\n",
       " 0.0008917941595427692,\n",
       " 0.0008940278203226626,\n",
       " 0.0008920307736843824,\n",
       " 0.000890937983058393,\n",
       " 0.0008917977102100849,\n",
       " 0.0008921835105866194,\n",
       " 0.0008922090637497604,\n",
       " 0.0008926052832975984,\n",
       " 0.0008918076055124402,\n",
       " 0.0008906674338504672,\n",
       " 0.0008930912590585649,\n",
       " 0.0008908621384762228,\n",
       " 0.0008916282677091658,\n",
       " 0.0008937186212278903,\n",
       " 0.000890945375431329,\n",
       " 0.0008907641167752445,\n",
       " 0.0008915438666008413,\n",
       " 0.0008914439240470529,\n",
       " 0.0008919878746382892,\n",
       " 0.0008926994632929564,\n",
       " 0.0008905489812605083,\n",
       " 0.0008911558543331921,\n",
       " 0.0008915979415178299,\n",
       " 0.0008918197127059102,\n",
       " 0.0008930416079238057,\n",
       " 0.0008910448523238301,\n",
       " 0.0008914386271499097,\n",
       " 0.0008909013122320175,\n",
       " 0.0008918761159293354,\n",
       " 0.00089509307872504,\n",
       " 0.0008911989280022681,\n",
       " 0.0008930753101594746,\n",
       " 0.0008915240177884698,\n",
       " 0.0008911369950510561,\n",
       " 0.0008909132448025048,\n",
       " 0.0008936223457567394,\n",
       " 0.0008927183807827532,\n",
       " 0.0008907845476642251,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([0,1,3,4,5,6])[0,-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04264463484287262,\n",
       " 0.0007411547121591866,\n",
       " 0.026228688657283783,\n",
       " 0.00030988658545538783,\n",
       " 0.0014420798979699612,\n",
       " 0.07105280458927155,\n",
       " 0.019089749082922935,\n",
       " 0.06734924763441086,\n",
       " 0.7528252005577087,\n",
       " 0.0012723421677947044,\n",
       " 0.002091397298499942,\n",
       " 0.00248482427559793,\n",
       " 6.402467988664284e-06,\n",
       " 1.0556628488034292e-11,\n",
       " 6.5118006205011625e-06,\n",
       " 3.794242697807704e-11,\n",
       " 1.211845301440917e-06,\n",
       " 1.291887291587912e-10,\n",
       " 1.117326977467803e-11,\n",
       " 1.1402470590837765e-05,\n",
       " 2.7290061552776024e-06,\n",
       " 3.981174450018443e-05,\n",
       " 2.3353580402840635e-08,\n",
       " 8.445293943337706e-12,\n",
       " 8.98866687748523e-08,\n",
       " 1.3542368615393485e-11,\n",
       " 2.7604359770228015e-11,\n",
       " 6.181338818578297e-08,\n",
       " 8.222898941312451e-06,\n",
       " 4.1774046621867456e-06,\n",
       " 1.4604727311962051e-06,\n",
       " 4.5961508021719055e-07,\n",
       " 3.097349113301284e-11,\n",
       " 4.011962744243647e-07,\n",
       " 4.1119378124676587e-08,\n",
       " 5.787887964042726e-12,\n",
       " 1.3024202827338627e-09,\n",
       " 1.293114226808001e-11,\n",
       " 3.2725504297559382e-06,\n",
       " 3.531746131102409e-07,\n",
       " 2.931322114818613e-06,\n",
       " 3.393749921087874e-07,\n",
       " 4.0059048842522316e-06,\n",
       " 7.4556701292749494e-06,\n",
       " 4.861989283400092e-11,\n",
       " 1.7826826820277475e-11,\n",
       " 2.5845681497771444e-11,\n",
       " 5.925079177293568e-11,\n",
       " 3.982911422895086e-08,\n",
       " 7.850384281482548e-05,\n",
       " 2.3345797517393407e-11,\n",
       " 2.664648536543357e-11,\n",
       " 2.3506240509618692e-08,\n",
       " 5.340085635907599e-07,\n",
       " 1.0421121032777592e-07,\n",
       " 1.1209236663867195e-11,\n",
       " 5.4287543210795874e-11,\n",
       " 9.826164770743162e-09,\n",
       " 4.1339626477565616e-07,\n",
       " 5.8550485846353695e-06,\n",
       " 3.3195917126249697e-07,\n",
       " 0.0012940701562911272,\n",
       " 4.641890427592443e-06,\n",
       " 8.980424581750412e-08,\n",
       " 1.402356019752915e-06,\n",
       " 1.3348049332506662e-09,\n",
       " 1.3322151062311605e-06,\n",
       " 3.9741826185490936e-07,\n",
       " 0.0015387649182230234,\n",
       " 0.0009473665850237012,\n",
       " 0.00011045896098949015,\n",
       " 0.00024060095893219113,\n",
       " 0.0005135777755640447,\n",
       " 3.86580650229007e-05,\n",
       " 1.4889924386807252e-06,\n",
       " 2.80623294202087e-06,\n",
       " 9.978477919503348e-07,\n",
       " 2.9402112744492115e-08,\n",
       " 9.319845162281126e-08,\n",
       " 4.035273448721455e-09,\n",
       " 2.606694304851942e-10,\n",
       " 1.1541786903990214e-07,\n",
       " 6.629911513300613e-05,\n",
       " 8.502734658577538e-07,\n",
       " 5.150983270141296e-06,\n",
       " 4.043764420202933e-05,\n",
       " 4.350749804871157e-06,\n",
       " 1.326701294601662e-06,\n",
       " 1.2995728582154698e-07,\n",
       " 5.807459046991426e-07,\n",
       " 9.415062436346489e-07,\n",
       " 0.0001382519694743678,\n",
       " 1.8489503190721734e-06,\n",
       " 1.205448029395484e-06,\n",
       " 4.054965302202618e-06,\n",
       " 4.6234068656891836e-11,\n",
       " 1.0593055078944591e-11,\n",
       " 1.3250952335397415e-08,\n",
       " 1.1213285233679926e-06,\n",
       " 9.028982503878069e-07,\n",
       " 1.389673620622034e-08,\n",
       " 1.0086789181684708e-09,\n",
       " 7.031942250712575e-10,\n",
       " 4.407672349771019e-06,\n",
       " 6.740378255543078e-10,\n",
       " 1.0187815746576234e-07,\n",
       " 6.253794140320679e-07,\n",
       " 4.992161393602146e-06,\n",
       " 8.807184599390894e-07,\n",
       " 3.1120299581743893e-07,\n",
       " 3.7766145766227055e-09,\n",
       " 8.647342286671744e-10,\n",
       " 9.007235061975205e-12,\n",
       " 1.9642585336043794e-09,\n",
       " 5.328272892346497e-11,\n",
       " 1.910587315023804e-07,\n",
       " 7.73834685219299e-08,\n",
       " 3.4753108479890216e-07,\n",
       " 7.558443826383154e-07,\n",
       " 1.4734968090124312e-07,\n",
       " 3.8219216236257125e-08,\n",
       " 1.6104570022434928e-05,\n",
       " 0.00025268265744671226,\n",
       " 1.2083986803190783e-05,\n",
       " 5.4565553000429645e-05,\n",
       " 0.00015531136887148023,\n",
       " 8.819974027574062e-06,\n",
       " 2.2248470941121923e-06,\n",
       " 0.00024518920690752566,\n",
       " 0.000320574501529336,\n",
       " 0.00042695540469139814,\n",
       " 1.1025798812624998e-05,\n",
       " 2.653727733559208e-06,\n",
       " 3.758135136422425e-08,\n",
       " 2.1960597678116756e-06,\n",
       " 1.8252731592838245e-07,\n",
       " 1.3233002391643822e-05,\n",
       " 1.1297363045059683e-08,\n",
       " 4.968320421738781e-09,\n",
       " 4.0195207162696533e-08,\n",
       " 2.8932749529531065e-08,\n",
       " 3.8736738261491155e-09,\n",
       " 5.012777819501935e-07,\n",
       " 1.1875937389049795e-06,\n",
       " 9.121845323534217e-08,\n",
       " 6.090298487748669e-09,\n",
       " 1.0097842562117876e-08,\n",
       " 1.044920483472822e-09,\n",
       " 4.300791545119864e-08,\n",
       " 7.262743878522215e-11,\n",
       " 3.599976935220184e-06,\n",
       " 3.38649321918183e-08,\n",
       " 6.859693257865729e-08,\n",
       " 7.084790354383586e-08,\n",
       " 2.7086790588271015e-09,\n",
       " 1.737284066605227e-10,\n",
       " 3.1961484570075527e-09,\n",
       " 3.0386917870828256e-08,\n",
       " 8.662748740562165e-09,\n",
       " 1.3669901322010958e-10,\n",
       " 2.6361790528284246e-10,\n",
       " 6.922711293100292e-10,\n",
       " 1.4783287927144784e-09,\n",
       " 0.00024381045659538358,\n",
       " 3.867584018735215e-05,\n",
       " 3.270726665505208e-05,\n",
       " 2.9331244149943814e-05,\n",
       " 1.2663758752751164e-06,\n",
       " 4.6978283307907986e-07,\n",
       " 0.0002265407529193908,\n",
       " 1.4256499980547233e-07,\n",
       " 2.752480440904037e-06,\n",
       " 2.7376840989745688e-06,\n",
       " 1.9185947621735977e-06,\n",
       " 1.4051077414478641e-05,\n",
       " 1.5956803736116854e-06,\n",
       " 5.932416570431087e-06,\n",
       " 2.5269312118325615e-06,\n",
       " 7.235627435875358e-06,\n",
       " 1.7642867078393465e-07,\n",
       " 2.7854252948600333e-06,\n",
       " 3.1420270829585206e-07,\n",
       " 1.566986429679673e-05,\n",
       " 3.840385761577636e-05,\n",
       " 4.1358518501510844e-05,\n",
       " 6.02133968641283e-06,\n",
       " 1.7833190213423222e-07,\n",
       " 6.190048793541791e-07,\n",
       " 3.578205962639913e-07,\n",
       " 3.0775311188335763e-07,\n",
       " 8.386768968193792e-06,\n",
       " 2.8717345230688807e-06,\n",
       " 1.7093611859309021e-07,\n",
       " 7.0799146101308e-08,\n",
       " 1.9985477592854295e-06,\n",
       " 5.063250500825234e-05,\n",
       " 1.5436725675499474e-07,\n",
       " 1.3108740404277341e-07,\n",
       " 6.852435774362675e-08,\n",
       " 2.787912478652288e-08,\n",
       " 4.298747580122608e-09,\n",
       " 3.4871584375650855e-06,\n",
       " 4.773835726723519e-08,\n",
       " 9.909808085240002e-09,\n",
       " 2.9235214249467845e-10,\n",
       " 7.760924680688674e-10,\n",
       " 2.1705388875137288e-10,\n",
       " 1.4641060933073824e-11,\n",
       " 0.00016488751862198114,\n",
       " 2.3154897443866673e-10,\n",
       " 3.8664939694399436e-08,\n",
       " 3.4635103673963386e-08,\n",
       " 5.384139800668208e-09,\n",
       " 3.752576915871941e-08,\n",
       " 1.520969590274035e-07,\n",
       " 8.692876463101129e-08,\n",
       " 1.182391184961773e-09,\n",
       " 1.906616375890735e-08,\n",
       " 5.77627581321849e-08,\n",
       " 6.414216380257187e-11,\n",
       " 5.896086396894873e-10,\n",
       " 1.148856183796898e-12,\n",
       " 1.002942917693872e-05,\n",
       " 4.3335515731435237e-10,\n",
       " 6.19452766859041e-10,\n",
       " 4.6790287200337843e-08,\n",
       " 5.0191011069511404e-11,\n",
       " 4.017038784942528e-11,\n",
       " 5.796975122152048e-10,\n",
       " 7.571442778298021e-11,\n",
       " 1.448755351796116e-11,\n",
       " 4.5108612312105834e-07,\n",
       " 2.920685915341892e-09,\n",
       " 4.257028951570874e-09,\n",
       " 3.010128535319012e-10,\n",
       " 7.095010801094759e-09,\n",
       " 1.1736099592152271e-10,\n",
       " 6.493348525538067e-09,\n",
       " 3.2464060328862843e-09,\n",
       " 0.00010426803055452183,\n",
       " 6.531435303713806e-08,\n",
       " 4.153678673901595e-05,\n",
       " 1.5133821307244943e-07,\n",
       " 6.880092087158118e-07,\n",
       " 3.1335837036294834e-08,\n",
       " 3.4639779755707423e-07,\n",
       " 3.1707651942269877e-05,\n",
       " 8.434860205852601e-07,\n",
       " 1.3233866411610506e-06,\n",
       " 4.605323283612961e-06,\n",
       " 1.736037120281253e-06,\n",
       " 1.8485094344100617e-08,\n",
       " 7.046221384143792e-08,\n",
       " 1.926200457091909e-05,\n",
       " 3.4189049813448946e-08,\n",
       " 9.470890738327853e-10,\n",
       " 9.520545063423924e-08,\n",
       " 2.7801384305803367e-08,\n",
       " 1.807482358229251e-10,\n",
       " 9.210443963070247e-09,\n",
       " 3.772050671813076e-09,\n",
       " 6.379757877539305e-08,\n",
       " 1.3061275616621515e-08,\n",
       " 5.860771423726874e-09,\n",
       " 1.2687729622484767e-06,\n",
       " 8.15496832728968e-07,\n",
       " 2.3039333996166533e-07,\n",
       " 2.4789301278360654e-08,\n",
       " 7.865551197028253e-07,\n",
       " 1.1492119966760583e-07,\n",
       " 2.857441359083168e-06,\n",
       " 1.0212431789113907e-06,\n",
       " 7.775022936584719e-08,\n",
       " 9.682947152422905e-12,\n",
       " 4.996803113499482e-07,\n",
       " 1.12306111077487e-08,\n",
       " 7.72234307078179e-06,\n",
       " 3.193300130988064e-07,\n",
       " 3.056671360468499e-08,\n",
       " 1.0995513832767756e-07,\n",
       " 9.285799933422823e-07,\n",
       " 3.762862021972069e-08,\n",
       " 7.30283966632328e-09,\n",
       " 1.1419516721122847e-10,\n",
       " 1.3674446108780103e-07,\n",
       " 2.2016180082573555e-05,\n",
       " 1.7259169737826596e-07,\n",
       " 3.86665815312881e-05,\n",
       " 1.7382355963491136e-06,\n",
       " 3.453409954090603e-05,\n",
       " 2.570476135588251e-05,\n",
       " 6.978637401289234e-08,\n",
       " 8.976892331702402e-07,\n",
       " 3.7611832794937072e-06,\n",
       " 1.430495103704743e-06,\n",
       " 1.6498746902016137e-07,\n",
       " 2.524395767977694e-07,\n",
       " 5.3186635341262445e-05,\n",
       " 2.908409157953429e-07,\n",
       " 2.3211379811982624e-05,\n",
       " 9.832792784436606e-07,\n",
       " 1.0725224797170085e-07,\n",
       " 5.327209692040924e-06,\n",
       " 4.3695342810678994e-07,\n",
       " 0.00013637311349157244,\n",
       " 1.2824795021515456e-06,\n",
       " 1.08712843172043e-08,\n",
       " 4.504310027186875e-07,\n",
       " 3.317046859407924e-09,\n",
       " 1.9736923206892243e-09,\n",
       " 2.7838407135050147e-09,\n",
       " 8.200478163189473e-08,\n",
       " 4.2755873841393566e-10,\n",
       " 1.0161795849228383e-08,\n",
       " 3.5396502084950043e-07,\n",
       " 7.891728728282033e-07,\n",
       " 3.151131693357456e-07,\n",
       " 3.123161036455713e-07,\n",
       " 2.1652547275152756e-06,\n",
       " 8.513834082179983e-09,\n",
       " 4.908156014948872e-08,\n",
       " 6.728323853621987e-08,\n",
       " 1.5972224931548595e-10,\n",
       " 3.439668272431362e-10,\n",
       " 4.030871469939967e-10,\n",
       " 3.2893197055017254e-09,\n",
       " 5.677477585286361e-12,\n",
       " 2.089931712134785e-07,\n",
       " 1.6230236710157442e-08,\n",
       " 5.00312546947157e-09,\n",
       " 1.1221661679883255e-06,\n",
       " 1.1160288249101313e-09,\n",
       " 1.8210012342478876e-07,\n",
       " 6.692872602798161e-07,\n",
       " 3.645039114985593e-08,\n",
       " 5.694301989933592e-07,\n",
       " 5.34393223006191e-07,\n",
       " 1.1391471161914524e-05,\n",
       " 3.049551082767721e-07,\n",
       " 2.4014190785237588e-05,\n",
       " 6.712289035704089e-08,\n",
       " 1.9932066663841397e-08,\n",
       " 4.221886626964988e-07,\n",
       " 2.728092773907065e-08,\n",
       " 5.411250114661925e-08,\n",
       " 7.133387924795898e-08,\n",
       " 1.1829299637611257e-06,\n",
       " 4.243304647388868e-05,\n",
       " 2.574818722678174e-07,\n",
       " 0.00022904157231096178,\n",
       " 1.0001118425861932e-05,\n",
       " 9.673779004515382e-07,\n",
       " 2.1534754068852635e-06,\n",
       " 0.00018905980687122792,\n",
       " 9.423461051483173e-07,\n",
       " 8.342332762367732e-07,\n",
       " 0.00019441491167526692,\n",
       " 0.0001421077613485977,\n",
       " 3.720450933997199e-07,\n",
       " 1.938994046213338e-06,\n",
       " 2.030306632150314e-06,\n",
       " 1.3226970452251408e-07,\n",
       " 8.844906318472567e-08,\n",
       " 8.643108913020114e-07,\n",
       " 2.8920230761286803e-06,\n",
       " 1.1813904166046996e-07,\n",
       " 1.4834180319667212e-06,\n",
       " 2.5722096097524627e-07,\n",
       " 9.073461981756736e-09,\n",
       " 3.8607808505730645e-07,\n",
       " 3.6920863522027503e-07,\n",
       " 1.134894276333398e-07,\n",
       " 5.115062322147423e-06,\n",
       " 5.255044399632425e-09,\n",
       " 7.285203196261136e-08,\n",
       " 6.397841843863716e-06,\n",
       " 9.004505585608058e-08,\n",
       " 7.84761050454108e-06,\n",
       " 6.979586260058568e-07,\n",
       " 1.4032596595825453e-07,\n",
       " 8.333760348477881e-09,\n",
       " 3.14596206862916e-08,\n",
       " 8.37929337027532e-10,\n",
       " 1.700700948958911e-07,\n",
       " 4.959376198598875e-08,\n",
       " 4.230842343133645e-09,\n",
       " 1.2328045784215647e-07,\n",
       " 2.9800974061799934e-06,\n",
       " 1.1721011716758767e-08,\n",
       " 6.175564237764775e-08,\n",
       " 5.150747051096971e-10,\n",
       " 3.8493813692142e-10,\n",
       " 1.2316988318161748e-07,\n",
       " 1.2741645605274243e-07,\n",
       " 1.9310928678351047e-07,\n",
       " 5.652604428973973e-08,\n",
       " 5.566814138546761e-07,\n",
       " 4.086277982651154e-08,\n",
       " 4.26667680136425e-13,\n",
       " 4.949342837790027e-07,\n",
       " 1.1135529348393902e-05,\n",
       " 1.7028227716764377e-08,\n",
       " 2.097489071317682e-09,\n",
       " 8.983292998365755e-10,\n",
       " 2.836598511635202e-09,\n",
       " 6.252935058626008e-09,\n",
       " 4.34895980561123e-07,\n",
       " 6.0311009519864456e-09,\n",
       " 6.070513425271429e-09,\n",
       " 3.6116617252446304e-08,\n",
       " 1.7845355614554137e-05,\n",
       " 4.10682987421751e-05,\n",
       " 2.6079353574459674e-06,\n",
       " 7.693376460338186e-07,\n",
       " 1.9448117427600664e-07,\n",
       " 5.331381558448811e-08,\n",
       " 6.51765930115289e-08,\n",
       " 4.481786618271144e-06,\n",
       " 3.2157359441953304e-08,\n",
       " 2.747687233295437e-07,\n",
       " 1.1047848857970166e-07,\n",
       " 9.033524796253634e-12,\n",
       " 8.071287084021606e-06,\n",
       " 6.780564376640541e-08,\n",
       " 5.831450948790007e-07,\n",
       " 9.590750238430701e-08,\n",
       " 3.3787257507356117e-06,\n",
       " 6.305314173005172e-07,\n",
       " 0.00011260888277320191,\n",
       " 1.0009300694946432e-06,\n",
       " 1.106686681850988e-06,\n",
       " 3.9195191448015976e-07,\n",
       " 3.15517425519829e-08,\n",
       " 7.861636390771309e-07,\n",
       " 7.417659162456403e-07,\n",
       " 6.328375457087532e-05,\n",
       " 5.251660439853367e-08,\n",
       " 1.714633945937294e-08,\n",
       " 8.548859536938735e-11,\n",
       " 4.184987005828589e-07,\n",
       " 7.744463914605149e-07,\n",
       " 3.6091758914835737e-08,\n",
       " 6.808193830920573e-08,\n",
       " 1.5868776293004316e-09,\n",
       " 5.216161369503425e-08,\n",
       " 2.0513368781394092e-09,\n",
       " 1.0644739312226648e-08,\n",
       " 6.385950879206348e-09,\n",
       " 6.851804457141952e-09,\n",
       " 5.548729031801258e-09,\n",
       " 8.096962211823211e-09,\n",
       " 2.4423091587316703e-12,\n",
       " 1.7765916515699587e-09,\n",
       " 2.6473714331842757e-09,\n",
       " 5.5555940292606465e-08,\n",
       " 3.9934622719783874e-08,\n",
       " 3.157313130941475e-07,\n",
       " 7.982123861438595e-06,\n",
       " 2.281190290887025e-06,\n",
       " 6.258256348701252e-07,\n",
       " 1.4941211702534929e-05,\n",
       " 5.023047151553328e-07,\n",
       " 1.2945001515163312e-07,\n",
       " 4.1672919337543135e-07,\n",
       " 1.919325853805276e-07,\n",
       " 2.978971513911688e-10,\n",
       " 2.933989995401731e-10,\n",
       " 1.4741136642511948e-13,\n",
       " 1.4775097811892124e-11,\n",
       " 3.211681359971408e-06,\n",
       " 5.7495764593795684e-08,\n",
       " 2.019169009770394e-08,\n",
       " 3.990326818970047e-11,\n",
       " 1.6511035363909153e-15,\n",
       " 9.17324399898672e-16,\n",
       " 6.839774786045888e-14,\n",
       " 3.0218171443840286e-14,\n",
       " 4.4865556446871055e-15,\n",
       " 5.6300841322354245e-08,\n",
       " 1.0620469481636974e-07,\n",
       " 4.866411805048033e-16,\n",
       " 9.05878497214907e-12,\n",
       " 2.2320645884295635e-15,\n",
       " 1.0988091513064457e-11,\n",
       " 1.007314381240576e-06,\n",
       " 5.8574997297000664e-08,\n",
       " 7.304957527765055e-09,\n",
       " 2.2760847059544176e-05,\n",
       " 1.1209944261736382e-07,\n",
       " 1.092187051909832e-08,\n",
       " 2.550863860417696e-11,\n",
       " 3.648262383382575e-14,\n",
       " 1.9883204457893378e-11,\n",
       " 8.757652381063963e-07,\n",
       " 5.257067003938687e-10,\n",
       " 1.8054649331133987e-08,\n",
       " 2.4819027544253913e-07,\n",
       " 4.0265341283429734e-08,\n",
       " 2.754030674623209e-07,\n",
       " 3.3218396922052307e-09,\n",
       " 1.1370213215400327e-08,\n",
       " 4.8286454695789516e-11,\n",
       " 1.4340986716229287e-15,\n",
       " 5.292953246574405e-22,\n",
       " 1.1959720576245884e-18,\n",
       " 2.2616468697833625e-08,\n",
       " 4.7615391629562964e-09,\n",
       " 1.4592188790629734e-07,\n",
       " 1.2105198265999206e-07,\n",
       " 4.789017489861824e-13,\n",
       " 1.2176919391706457e-14,\n",
       " 1.0230439828617932e-09,\n",
       " 4.535039132961584e-14,\n",
       " 1.1674369488900993e-06,\n",
       " 5.725508955833902e-09,\n",
       " 2.6701509892035347e-09,\n",
       " 6.0989264853705816e-12,\n",
       " 4.397611615136743e-12,\n",
       " 5.211167035668041e-07,\n",
       " 1.3492545747340046e-07,\n",
       " 6.693647947031423e-08,\n",
       " 1.842570327426074e-07,\n",
       " 2.914331389547442e-07,\n",
       " 3.002155679610041e-08,\n",
       " 7.136859039746923e-06,\n",
       " 2.3244805902322696e-07,\n",
       " 8.287199956729889e-18,\n",
       " 3.36642987846626e-17,\n",
       " 1.1420062264733133e-06,\n",
       " 3.202856646566943e-08,\n",
       " 3.3854277603495575e-08,\n",
       " 8.964965658719848e-09,\n",
       " 1.3888023543684458e-11,\n",
       " 3.986755370277706e-09,\n",
       " 1.82765150236186e-10,\n",
       " 1.9141343443607184e-07,\n",
       " 8.152040464892707e-08,\n",
       " 6.392608042915526e-08,\n",
       " 1.3245419268059777e-06,\n",
       " 6.100189693825087e-06,\n",
       " 2.673417043297377e-13,\n",
       " 2.317546200554402e-16,\n",
       " 3.0095782065104736e-17,\n",
       " 3.4702759854507393e-15,\n",
       " 5.771754740813151e-10,\n",
       " 6.696422474306019e-07,\n",
       " 1.8285328451383975e-07,\n",
       " 1.58911007019924e-07,\n",
       " 0.0001626181328902021,\n",
       " 1.3664865036844276e-05,\n",
       " 4.815661668544635e-06,\n",
       " 4.221403457904671e-07,\n",
       " 2.2433939648180967e-06,\n",
       " 6.265906904445728e-07,\n",
       " 1.8344733689446002e-05,\n",
       " 4.7935242037056014e-05,\n",
       " 1.1553960913013306e-12,\n",
       " 2.3483619315811666e-07,\n",
       " 4.790633283846546e-06,\n",
       " 1.360348544920953e-08,\n",
       " 3.0291131825066486e-09,\n",
       " 2.9193589212717086e-10,\n",
       " 2.0735589600917592e-07,\n",
       " 3.1705265428172424e-05,\n",
       " 1.0347279157940648e-06,\n",
       " 1.3747867910751665e-07,\n",
       " 2.96326447823958e-07,\n",
       " 5.541172640732839e-07,\n",
       " 1.77643769916358e-07,\n",
       " 1.2937533711010474e-07,\n",
       " 9.08382605246949e-11,\n",
       " 8.318721711475519e-09,\n",
       " 1.918816039392368e-08,\n",
       " 1.0583898074401077e-05,\n",
       " 8.947678225013078e-07,\n",
       " 1.2122363876598818e-11,\n",
       " 3.0344941137627757e-07,\n",
       " 1.5703777478393022e-07,\n",
       " 4.1949004980779137e-07,\n",
       " 1.291383682655578e-06,\n",
       " 1.0607062677081558e-06,\n",
       " 8.498150236846413e-06,\n",
       " 8.325341332238168e-05,\n",
       " 1.6882216513636195e-08,\n",
       " 2.4701948819760844e-11,\n",
       " 5.5298411183457574e-08,\n",
       " 6.3127026805887e-07,\n",
       " 5.570353422967855e-08,\n",
       " 2.571069614987209e-07,\n",
       " 6.786720518903167e-07,\n",
       " 1.4696142898174003e-06,\n",
       " 1.0770627341116779e-06,\n",
       " 5.205072852731973e-07,\n",
       " 6.3687184592708945e-06,\n",
       " 1.2674129523659872e-09,\n",
       " 8.000105253813672e-07,\n",
       " 2.5302734130150384e-08,\n",
       " 1.7903270190799958e-06,\n",
       " 2.7430453997112636e-07,\n",
       " 3.49179174463643e-09,\n",
       " 1.3061848491702222e-08,\n",
       " 2.0280482848633596e-10,\n",
       " 4.387754881651773e-10,\n",
       " 2.203463784046858e-13,\n",
       " 2.5497362061326426e-14,\n",
       " 5.516640096125958e-12,\n",
       " 9.108205301089356e-10,\n",
       " 2.6070323677629403e-10,\n",
       " 4.3440345071088515e-11,\n",
       " 1.0621076995676049e-07,\n",
       " 1.7763291282335558e-07,\n",
       " 1.0392894322919233e-09,\n",
       " 6.4622369677636016e-09,\n",
       " 6.051418921515506e-08,\n",
       " 5.80186224397039e-07,\n",
       " 7.371071530926088e-10,\n",
       " 3.0023776798060453e-09,\n",
       " 5.956994897360346e-09,\n",
       " 8.180839686233732e-12,\n",
       " 6.961387033228156e-14,\n",
       " 3.433099290961472e-11,\n",
       " 1.9327046907324785e-11,\n",
       " 8.980097915278762e-12,\n",
       " 7.284917669103663e-10,\n",
       " 2.373460894489199e-08,\n",
       " 9.925824606682454e-10,\n",
       " 5.920622658805996e-09,\n",
       " 2.229313622592599e-06,\n",
       " 5.872632868886285e-07,\n",
       " 2.3448731667485845e-07,\n",
       " 9.079875740169996e-11,\n",
       " 8.036705524162857e-14,\n",
       " 5.760159710321844e-11,\n",
       " 1.7580854549947844e-09,\n",
       " 1.0525595950383604e-09,\n",
       " 4.3801781320941535e-14,\n",
       " 2.769003379435715e-16,\n",
       " 6.40243170790025e-19,\n",
       " 1.3359884758808865e-14,\n",
       " 1.549461448036027e-09,\n",
       " 3.881136301231436e-09,\n",
       " 7.890313895586587e-07,\n",
       " 4.895228471468727e-07,\n",
       " 3.22585975709444e-08,\n",
       " 2.757937767050578e-10,\n",
       " 8.56248192349085e-08,\n",
       " 1.5732481983832258e-07,\n",
       " 1.575337364556617e-06,\n",
       " 1.6724061424611136e-07,\n",
       " 1.1267778887713575e-07,\n",
       " 1.374338438608902e-07,\n",
       " 1.1409279210283785e-07,\n",
       " 3.89823178181814e-08,\n",
       " 3.333588516341024e-09,\n",
       " 2.0130386246819398e-07,\n",
       " 1.5817520306882216e-06,\n",
       " 5.788686507912644e-07,\n",
       " 4.8917718833974e-12,\n",
       " 1.4463857667124103e-08,\n",
       " 6.5039338359440535e-09,\n",
       " 1.5668479846908667e-08,\n",
       " 5.82075017518946e-07,\n",
       " 1.3843759916198906e-06,\n",
       " 2.2918739261967858e-07,\n",
       " 3.16456935252063e-05,\n",
       " 1.87003514895423e-08,\n",
       " 2.4480726334985548e-09,\n",
       " 2.4665969249326736e-06,\n",
       " 7.955846648144416e-09,\n",
       " 4.3236319413608726e-08,\n",
       " 8.243049798295488e-09,\n",
       " 9.410538268639357e-07,\n",
       " 0.00028921294142492115,\n",
       " 9.41528287512483e-06,\n",
       " 1.921451257658191e-06,\n",
       " 2.7998374207527377e-05,\n",
       " 2.7346804927219637e-06,\n",
       " 2.2704435309606197e-07,\n",
       " 1.1351362445566338e-06,\n",
       " 5.0003854994429275e-06,\n",
       " 1.1898174307134468e-05,\n",
       " 8.059457286435645e-06,\n",
       " 5.772327313025016e-06,\n",
       " 2.9936761620774632e-06,\n",
       " 3.536522115155094e-07,\n",
       " 1.7036163058037346e-07,\n",
       " 8.745971172174904e-06,\n",
       " 3.87084401154425e-06,\n",
       " 2.458880317135481e-07,\n",
       " 2.6528229568612005e-07,\n",
       " 1.4460630914925332e-08,\n",
       " 2.64753430201381e-06,\n",
       " 6.937284524610732e-06,\n",
       " 3.327098241356907e-08,\n",
       " 9.20270437632098e-08,\n",
       " 8.862540994414303e-07,\n",
       " 3.1251859127223724e-06,\n",
       " 1.6303529548622464e-07,\n",
       " 3.474909471279375e-09,\n",
       " 5.3877720347372815e-05,\n",
       " 1.942174350233472e-07,\n",
       " 1.7773831473277824e-07,\n",
       " 7.758702122373506e-05,\n",
       " 0.00030501617584377527,\n",
       " 1.9642952793219592e-06,\n",
       " 2.930960363300983e-05,\n",
       " 0.0003598794573917985,\n",
       " 1.8641717360878829e-06,\n",
       " 3.431924540109321e-07,\n",
       " 3.925006603822112e-05,\n",
       " 3.73335501535621e-06,\n",
       " 1.6397688114011544e-06,\n",
       " 0.00040951886330731213,\n",
       " 1.1915513823623769e-05,\n",
       " 2.7702362785930745e-05,\n",
       " 0.00011458648805273697,\n",
       " 4.868024007009808e-06,\n",
       " 1.030502789944876e-05,\n",
       " 7.182630361057818e-05,\n",
       " 3.0622938993474236e-06,\n",
       " 1.114841211347084e-06,\n",
       " 0.00021291033772286028,\n",
       " 8.077366510406137e-05,\n",
       " 2.7768041036324576e-05,\n",
       " 9.628325642552227e-06,\n",
       " 4.835466461372562e-05,\n",
       " 6.056762344996969e-07,\n",
       " 4.807821483154839e-07,\n",
       " 4.900871317659039e-07,\n",
       " 2.1833734820120299e-10,\n",
       " 8.335223178335127e-09,\n",
       " 1.3922310415637185e-08,\n",
       " 2.3184101394235768e-07,\n",
       " 1.4302297302037914e-07,\n",
       " 4.8392762019844326e-11,\n",
       " 2.9755391039998358e-08,\n",
       " 1.0689381671480369e-11,\n",
       " 2.9033587907179026e-06,\n",
       " 1.1875340988343197e-11,\n",
       " 3.791906237893272e-07,\n",
       " 2.6338923362345668e-06,\n",
       " 4.744480520457728e-06,\n",
       " 2.2976404068231204e-07,\n",
       " 5.3030384151497856e-05,\n",
       " 5.253809831629042e-06,\n",
       " 9.100590940558373e-12,\n",
       " 8.081212782284908e-12,\n",
       " 8.989146232929457e-12,\n",
       " 1.0544514046639808e-11,\n",
       " 9.171476945957213e-12,\n",
       " 1.450697368454712e-06,\n",
       " 8.829349098959938e-06,\n",
       " 0.00014924927381798625,\n",
       " 1.7238270402231137e-06,\n",
       " 8.4798687169374e-12,\n",
       " 1.2064836109326738e-11,\n",
       " 1.1051841733444867e-11,\n",
       " 9.848008693247312e-12,\n",
       " 1.2218884758163906e-11,\n",
       " 1.1020245480053426e-11,\n",
       " 1.0840084906094116e-11,\n",
       " 1.2075516801768327e-11,\n",
       " 1.3589616411346928e-11,\n",
       " 8.080535372767539e-12,\n",
       " 1.358463948969435e-11,\n",
       " 7.027577738488722e-12,\n",
       " 9.152150391711356e-12,\n",
       " 1.168975940352146e-11,\n",
       " 1.1003905252271462e-11,\n",
       " 8.233562269477357e-12,\n",
       " 1.238233907768782e-11,\n",
       " 1.4718001123403823e-11,\n",
       " 1.0087683292858696e-11,\n",
       " 1.1514124723110974e-11,\n",
       " 1.1543679206971191e-11,\n",
       " 1.1271775179899635e-11,\n",
       " 1.1740346542166158e-11,\n",
       " 8.3477634527096e-12,\n",
       " 1.0386185834987405e-11,\n",
       " 7.471905039135862e-12,\n",
       " 8.405344129047698e-12,\n",
       " 1.1195283415588175e-11,\n",
       " 1.3249568109330312e-11,\n",
       " 1.13568017837129e-11,\n",
       " 8.158355067899858e-12,\n",
       " 1.2802448337567718e-11,\n",
       " 8.756744461491106e-12,\n",
       " 1.0532332818391499e-11,\n",
       " 1.1400076195544617e-11,\n",
       " 1.090193647163007e-11,\n",
       " 1.1965786868295414e-11,\n",
       " 1.140579644620665e-11,\n",
       " 1.1817700464045178e-11,\n",
       " 1.489819725919439e-11,\n",
       " 1.2732656075320481e-11,\n",
       " 7.972589602389668e-12,\n",
       " 1.2534249679840848e-11,\n",
       " 9.569589912161724e-12,\n",
       " 1.1317597033155824e-11,\n",
       " 6.524797629275936e-12,\n",
       " 1.1005038026701275e-11,\n",
       " 1.2827330343745391e-11,\n",
       " 7.215958733319816e-12,\n",
       " 1.2415304895263546e-11,\n",
       " 1.0471280093016233e-11,\n",
       " 1.582224802565424e-11,\n",
       " 6.937137930068671e-12,\n",
       " 1.0517437615265024e-11,\n",
       " 1.1089363802230245e-11,\n",
       " 1.017601714697891e-11,\n",
       " 9.49202261929516e-12,\n",
       " 1.0972969928524368e-11,\n",
       " 1.0382679958842456e-11,\n",
       " 1.143221541738404e-11,\n",
       " 9.650349096224087e-12,\n",
       " 1.3161913399450942e-11,\n",
       " 8.182446907534224e-12,\n",
       " 1.1440504793513995e-11,\n",
       " 9.676835721617039e-12,\n",
       " 1.1413414484351403e-11,\n",
       " 1.1074440843528155e-11,\n",
       " 1.4695026445687986e-11,\n",
       " 1.157187453498798e-11,\n",
       " 1.5203487774284596e-11,\n",
       " 1.0962175611695102e-11,\n",
       " 9.639569524544367e-12,\n",
       " 9.19361548695763e-12,\n",
       " 1.0771904201956062e-11,\n",
       " 1.3097578577259128e-11,\n",
       " 9.45178917771683e-12,\n",
       " 1.0947695874841123e-11,\n",
       " 9.453934163294875e-12,\n",
       " 9.535899847534779e-12,\n",
       " 1.1437930463875645e-11,\n",
       " 1.2611420588393152e-11,\n",
       " 1.290175258295001e-11,\n",
       " 1.3027045588087649e-11,\n",
       " 1.3536506117406422e-11,\n",
       " 1.0510779746564225e-11,\n",
       " 9.654289520599768e-12,\n",
       " 8.160673525825501e-12,\n",
       " 1.0403652765667015e-11,\n",
       " 1.4422569041827593e-11,\n",
       " 9.163573545800663e-12,\n",
       " 9.602061333546796e-12,\n",
       " 1.1809970536236225e-11,\n",
       " 8.123294571726891e-12,\n",
       " 1.0110971955523684e-11,\n",
       " 9.198071124205676e-12,\n",
       " 7.941432234037649e-12,\n",
       " 8.372606427609064e-12,\n",
       " 1.3272584420409572e-11,\n",
       " 1.1595603817415867e-11,\n",
       " 9.481979437730992e-12,\n",
       " 1.0473357424378715e-11,\n",
       " 1.135864319268265e-11,\n",
       " 1.064401258105141e-11,\n",
       " 8.18575589256465e-12,\n",
       " 1.079819393623449e-11,\n",
       " 1.104773130616854e-11,\n",
       " 1.2008724743772792e-11,\n",
       " 1.011494533964541e-11,\n",
       " 9.909528059237616e-12,\n",
       " 7.439935820197086e-12,\n",
       " 9.072064280357672e-12,\n",
       " 9.863084307615289e-12,\n",
       " 1.0060110730569782e-11,\n",
       " 1.179709368387405e-11,\n",
       " 7.176346756426755e-12,\n",
       " 1.2899906837171571e-11,\n",
       " 8.904624433647701e-12,\n",
       " 1.0707869486925592e-11,\n",
       " 9.36943751750352e-12,\n",
       " 9.972877558495075e-12,\n",
       " 9.326360864148064e-12,\n",
       " 1.118254447374234e-11,\n",
       " 8.666767824239141e-12,\n",
       " 1.5696961092048767e-11,\n",
       " 8.01120107751796e-12,\n",
       " 1.2266357200807487e-11,\n",
       " 9.876939544017915e-12,\n",
       " 1.0267395440799465e-11,\n",
       " 1.044291823154575e-11,\n",
       " 1.110574653073737e-11,\n",
       " 8.670173953784222e-12,\n",
       " 1.147135771789598e-11,\n",
       " 1.0187940768791037e-11,\n",
       " 1.5241523321218864e-11,\n",
       " 6.8397817791499005e-12,\n",
       " 8.609037961682109e-12,\n",
       " 9.281340453137776e-12,\n",
       " 8.304836852934816e-12,\n",
       " 1.0736848909953522e-11,\n",
       " 1.1155658861949913e-11,\n",
       " 1.3604035432879247e-11,\n",
       " 9.681525546534342e-12,\n",
       " 7.719815238449446e-12,\n",
       " 8.114699016903426e-12,\n",
       " 7.544942969006652e-12,\n",
       " 9.452907206997097e-12,\n",
       " 9.752081087111009e-12,\n",
       " 1.0689300139476998e-11,\n",
       " 1.0323655992572345e-11,\n",
       " 1.2531404733340246e-11,\n",
       " 1.4881667079191807e-11,\n",
       " 1.2199068144536085e-11,\n",
       " 9.049321188225878e-12,\n",
       " 9.437233113029908e-12,\n",
       " 1.3864679369868238e-11,\n",
       " 8.38219337689905e-12,\n",
       " 1.2670539097092703e-11,\n",
       " 1.3109500464347779e-11,\n",
       " 1.1648561455690487e-11,\n",
       " 9.136209150328867e-12,\n",
       " 1.21265714483898e-11,\n",
       " 1.0075107415019602e-11,\n",
       " 1.2687613112905005e-11,\n",
       " 8.82296753018652e-12,\n",
       " 9.474892224969889e-12,\n",
       " 1.0033186954860884e-11,\n",
       " 1.3628396154652389e-11,\n",
       " 8.405857607196587e-12,\n",
       " 1.3493457219626581e-11,\n",
       " 8.61814005576056e-12,\n",
       " 1.049513427553439e-11,\n",
       " 1.2289423818867551e-11,\n",
       " 8.602062638585206e-12,\n",
       " 1.2221145102853104e-11,\n",
       " 1.6951304943058076e-11,\n",
       " 8.465582401750993e-12,\n",
       " 9.137429528294216e-12,\n",
       " 1.0161993642399114e-11,\n",
       " 9.905880803129374e-12,\n",
       " 7.987521234709138e-12,\n",
       " 8.276768159731773e-12,\n",
       " 1.2692477277531644e-11,\n",
       " 9.587347409023561e-12,\n",
       " 1.0614818052312458e-11,\n",
       " 7.583897919383187e-12,\n",
       " 1.1989729521710846e-11,\n",
       " 1.2442402143320042e-11,\n",
       " 1.0800006722266886e-11,\n",
       " 1.0140714656881045e-11,\n",
       " 7.039611948922442e-12,\n",
       " 1.0903537621398396e-11,\n",
       " 1.1168815872153459e-11,\n",
       " 8.467714376902968e-12,\n",
       " 6.374502054679043e-12,\n",
       " 1.2116561226571676e-11,\n",
       " 1.2071740308761125e-11,\n",
       " 1.078732415893402e-11,\n",
       " 6.432320821814219e-12,\n",
       " 1.3203728041477625e-11,\n",
       " 9.379450341406859e-12,\n",
       " 1.1411737006750133e-11,\n",
       " 1.1453211642975525e-11,\n",
       " 9.788008945021964e-12,\n",
       " 1.1296268608018689e-11,\n",
       " 8.137483742398643e-12,\n",
       " 1.105941206669403e-11,\n",
       " 9.464200256825706e-12,\n",
       " 1.1370499160279213e-11,\n",
       " 1.0848751584580096e-11,\n",
       " 8.442169706357472e-12,\n",
       " 1.181948029033153e-11,\n",
       " 7.334167127781566e-12,\n",
       " 8.637854320703298e-12,\n",
       " 9.23667739516354e-12,\n",
       " 1.3562840954495226e-11,\n",
       " 1.0940828104599731e-11,\n",
       " 1.4254132596480673e-11,\n",
       " 1.4590323740848454e-11,\n",
       " 1.2536186498601776e-11,\n",
       " 7.701755032341051e-12,\n",
       " 1.0075260938047226e-11,\n",
       " 1.0122955425295732e-11,\n",
       " 1.0622230525725307e-11,\n",
       " 1.015321680897241e-11,\n",
       " 1.3728055150985519e-11,\n",
       " 1.437478261423486e-11,\n",
       " 1.15624315677465e-11,\n",
       " 1.552415140881891e-11,\n",
       " 1.1322649415279606e-11,\n",
       " 1.1243990113984914e-11,\n",
       " 1.0859268345653206e-11,\n",
       " 8.390015245052229e-12,\n",
       " 8.484010369236294e-12,\n",
       " 1.1113418345309878e-11,\n",
       " 1.073641869853148e-11,\n",
       " 9.974056303097001e-12,\n",
       " 1.0911943224001242e-11,\n",
       " 9.126699396233562e-12,\n",
       " 8.388111386037345e-12,\n",
       " 6.539225324425635e-12,\n",
       " 8.955125703480338e-12,\n",
       " 9.596421747526396e-12,\n",
       " 1.043395838479233e-11,\n",
       " 8.654726241230648e-12,\n",
       " 6.836351363476156e-12,\n",
       " 1.3883705816952752e-11,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0,1,2,3,4,5,6]).reshape(1,-1)\n",
    "s = np.zeros(x.shape)\n",
    "model2.predict([x,s])[0,-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n",
      "991\n",
      "78\n",
      "492\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "964\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "457\n",
      "WARNING:tensorflow:7 out of the last 8 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "423\n",
      "WARNING:tensorflow:8 out of the last 9 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "394\n",
      "WARNING:tensorflow:9 out of the last 10 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "423\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "730\n",
      "WARNING:tensorflow:11 out of the last 12 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "1043\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "691\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "335\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "194\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "308\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "308\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "308\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "253\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "156\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "213\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "991\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "532\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "213\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "1077\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "277\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "510\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "958\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "807\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "1032\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "629\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "329\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "661\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "199\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "749\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "13\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "4\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "4\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "79\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "1056\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "43\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "442\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "323\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "442\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "191\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "323\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "287\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "1095\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "730\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "800\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "730\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "800\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "730\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "800\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "957\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "730\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "1031\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "957\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "835\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "835\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "835\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "964\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "247\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fdb7521a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "proposed_traj = [0]\n",
    "while len(proposed_traj) < 1000 and (len(proposed_traj) == 1 or proposed_traj[-1] != 0):\n",
    "    x = np.array(proposed_traj).reshape(1,-1)\n",
    "    #run with s = np.ones(x.shpae) for successful\n",
    "    #s = np.zeros(x.shape)\n",
    "    #predicted = np.argmax(model2.predict([x,s])[0,-1])\n",
    "    predicted = np.argmax(model.predict(x)[0,-1])\n",
    "    print(predicted)\n",
    "    proposed_traj.append(predicted)\n",
    "print(proposed_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node list of all students' learning pathway networks\n",
    "AM_nodelist = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-nodes-cohort.csv')\n",
    "AM_nodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendix to the node list that provides a set of XY coordinates to generate a common layout for all networks \n",
    "# produced in the analysis.  force atlas with parameterization <- what is this?\n",
    "AM_node_coord = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-nodes-coordinates-FA2.csv')\n",
    "AM_node_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student identifiers and performance statistics, certification, and enrollment data\n",
    "AM_id_and_performance = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "AM_id_and_performance['certGrp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta data includes the course title, run dates\n",
    "LaaL_meta = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-meta.csv')\n",
    "LaaL_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# complete course structure and module descriptions\n",
    "# list of student identifiers and performance statistics, certification, and enrollment data\n",
    "\n",
    "# LaaL_edgelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-modules.csv')\n",
    "# LaaL_edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_edelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-edges.csv')\n",
    "LaaL_edelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_nodelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-nodes.csv')\n",
    "LaaL_nodelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_node_coord = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-nodes-coordinates-FA2.csv')\n",
    "LaaL_node_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_id_and_performance = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO-LASERxB1-1T2019-auth_user-students.csv')\n",
    "LaaL_id_and_performance[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
