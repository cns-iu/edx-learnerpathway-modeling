{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, Reshape, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from data_parsing import load_trajectories\n",
    "from functools import partial, reduce\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "# from glob import glob\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trajectories, status = load_trajectories('data/MITxPRO+AMxB+1T2018/edges', 'data/MITxPRO+AMxB+1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "trajectories, status = load_trajectories('data/MITxPRO+LASERxB1+1T2019/LaaL', 'data/MITxPRO+LASERxB1+1T2019/MITxPRO-LASERxB1-1T2019-auth_user-students.csv')\n",
    "#trajectories = load_trajectories('data/MITxPRO+LASERxB1+1T2019/LaaL')\n",
    "#id_and_performance = pd.read_csv('data/MITxPRO-AMxB-1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "#id_and_performance = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "#id_and_performance.iloc[0]\n",
    "#id_and_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node list of all students' learning pathway networks\n",
    "# AM_nodelist = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-nodes-cohort.csv')\n",
    "#url_id_to_url_hexname = dict(zip(AM_nodelist['order'], AM_nodelist['id']))\n",
    "LaaL_nodelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-nodes.csv')\n",
    "url_id_to_url_hexname = dict(zip(LaaL_nodelist['order'], LaaL_nodelist['name']))\n",
    "url_id_to_url_hexname[0] = 'done with course'\n",
    "def traj_to_edge_csv(traj, fname):\n",
    "    edges = []\n",
    "    urls = [url_id_to_url_hexname[url] for url in traj][1:-1]\n",
    "    for edge in zip(urls[:-1], urls[1:]):\n",
    "            # unpack those two items in the in the pair, and convert them from idex id\n",
    "            edges.append(edge)\n",
    "    df = pd.DataFrame(data=edges, columns = ['from', 'to'])\n",
    "    df.to_csv(fname, index=False)\n",
    "#traj_to_edge_csv(trajectories.iloc[0], 'test_traj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AM_modules = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-modules.txt', sep='\\t', encoding='utf-16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum trajectory identified by number of clicks:  1699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count of students')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "traj_lengths = trajectories.map(len).values\n",
    "plt.hist(traj_lengths)\n",
    "print(\"Maximum trajectory identified by number of clicks: \", max(traj_lengths))\n",
    "plt.title('AM course')\n",
    "plt.xlabel('Learner trajectory length identified by number of clicks')\n",
    "plt.ylabel('Count of students')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06190476190476191"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.arange(len(trajectories))\n",
    "np.random.seed(9)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "incoming_traj = []\n",
    "outgoing_traj = []\n",
    "\n",
    "# this split assumes that we don't need to rebalance for unequal category weights\n",
    "n_valid = int(2*np.sqrt(len(trajectories)))\n",
    "n_train = len(trajectories) - n_valid\n",
    "success_rate = status[index[:n_train]].sum() / n_train\n",
    "\n",
    "for traj in trajectories.values:\n",
    "    incoming_traj.append(np.array(traj[:-1]).reshape(1,-1))\n",
    "    outgoing_traj.append(np.array(traj[1:]).reshape(-1,1))\n",
    "\n",
    "def data_generator(start, stop, use_status):\n",
    "     while True:\n",
    "        for i in range(start, stop):        \n",
    "            x = incoming_traj[index[i]].reshape(1,-1)\n",
    "            s = np.broadcast_to(status[index[i]], x.shape)\n",
    "            y = outgoing_traj[index[i]].reshape(1,-1)\n",
    "            if use_status:\n",
    "                yield [x,s],y\n",
    "            else:\n",
    "                yield x,y\n",
    "\n",
    "train_generator_simp = partial(data_generator, 0, n_train, False)\n",
    "valid_generator_simp = partial(data_generator, n_train, n_train+n_valid, False)\n",
    "train_generator_cond = partial(data_generator, 0, n_train, True)\n",
    "valid_generator_cond = partial(data_generator, n_train, n_train+n_valid, True)\n",
    "success_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 60\n",
    "embedding_dim = 30\n",
    "# turning trajectories into sets of URLs\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "number_of_URL = max(trajectories.sum()) + 1\n",
    "#number_of_URL = 1121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "history (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "URL_embedding (Embedding)    (None, None, 30)          13980     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, None, 60)          21840     \n",
      "_________________________________________________________________\n",
      "Predicted_URL (Dense)        (None, None, 466)         28426     \n",
      "=================================================================\n",
      "Total params: 64,246\n",
      "Trainable params: 64,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ = Input(shape=(None,), name='history')\n",
    "embed = Embedding(number_of_URL, embedding_dim, name='URL_embedding')(input_)\n",
    "\n",
    "rnn = LSTM(hidden_dim, return_sequences=True, name='LSTM')(embed)\n",
    "\n",
    "predicted_URL = Dense(number_of_URL, activation = 'softmax', name='Predicted_URL')(rnn)\n",
    "\n",
    "model_simp = Model(inputs=input_, outputs=predicted_URL, name='Simple_model')\n",
    "model_simp.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "model_simp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conditional_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "history (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "URL_embedding (Embedding)       (None, None, 30)     13980       history[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "status (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM (LSTM)                     (None, None, 60)     21840       URL_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Status_embedding (Embedding)    (None, None, 60)     120         status[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, None, 60)     0           LSTM[0][0]                       \n",
      "                                                                 Status_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Predicted_URL (Dense)           (None, None, 466)    28426       multiply[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 64,366\n",
      "Trainable params: 64,366\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_x = Input(shape=(None,), name='history')\n",
    "input_s = Input(shape=(None,), name='status')\n",
    "embed_x = Embedding(number_of_URL, embedding_dim, name='URL_embedding')(input_x)\n",
    "embed_s = Embedding(2, hidden_dim, embeddings_initializer='ones', name='Status_embedding')(input_s)\n",
    "\n",
    "rnn = LSTM(hidden_dim, return_sequences=True, name='LSTM')(embed_x)\n",
    "masked = Multiply()([rnn, embed_s])\n",
    "\n",
    "predicted_URL = Dense(number_of_URL, activation = 'softmax', name='Predicted_URL')(masked)\n",
    "\n",
    "model_cond = Model(inputs=[input_x, input_s], outputs=predicted_URL, name='Conditional_model')\n",
    "model_cond.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "model_cond.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_simp.load_weights('weights_simp.h5')\n",
    "#model_cond.load_weights('AM_cond_hiddim30-.hdf5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 210 steps, validate for 31 steps\n",
      "Epoch 1/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 5.0860 - acc: 0.0428\n",
      "Epoch 00001: val_loss improved from inf to 4.41791, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 54s 257ms/step - loss: 5.0838 - acc: 0.0430 - val_loss: 4.4179 - val_acc: 0.0769\n",
      "Epoch 2/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 3.7104 - acc: 0.1159\n",
      "Epoch 00002: val_loss improved from 4.41791 to 3.22363, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 52s 248ms/step - loss: 3.7089 - acc: 0.1162 - val_loss: 3.2236 - val_acc: 0.1641\n",
      "Epoch 3/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 2.7996 - acc: 0.2511\n",
      "Epoch 00003: val_loss improved from 3.22363 to 2.54444, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 53s 251ms/step - loss: 2.7994 - acc: 0.2515 - val_loss: 2.5444 - val_acc: 0.3328\n",
      "Epoch 4/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 2.2311 - acc: 0.4021\n",
      "Epoch 00004: val_loss improved from 2.54444 to 2.07830, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 60s 288ms/step - loss: 2.2315 - acc: 0.4023 - val_loss: 2.0783 - val_acc: 0.4499\n",
      "Epoch 5/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.8632 - acc: 0.4935\n",
      "Epoch 00005: val_loss improved from 2.07830 to 1.79848, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 64s 305ms/step - loss: 1.8640 - acc: 0.4935 - val_loss: 1.7985 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.6406 - acc: 0.5537\n",
      "Epoch 00006: val_loss improved from 1.79848 to 1.63353, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 61s 288ms/step - loss: 1.6416 - acc: 0.5536 - val_loss: 1.6335 - val_acc: 0.5700\n",
      "Epoch 7/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.5020 - acc: 0.5911\n",
      "Epoch 00007: val_loss improved from 1.63353 to 1.53249, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 53s 252ms/step - loss: 1.5030 - acc: 0.5909 - val_loss: 1.5325 - val_acc: 0.5969\n",
      "Epoch 8/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.4148 - acc: 0.6098\n",
      "Epoch 00008: val_loss improved from 1.53249 to 1.46765, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 245ms/step - loss: 1.4159 - acc: 0.6096 - val_loss: 1.4677 - val_acc: 0.6096\n",
      "Epoch 9/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.3546 - acc: 0.6220\n",
      "Epoch 00009: val_loss improved from 1.46765 to 1.42196, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 52s 249ms/step - loss: 1.3556 - acc: 0.6218 - val_loss: 1.4220 - val_acc: 0.6229\n",
      "Epoch 10/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.3100 - acc: 0.6309\n",
      "Epoch 00010: val_loss improved from 1.42196 to 1.38745, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 244ms/step - loss: 1.3110 - acc: 0.6307 - val_loss: 1.3875 - val_acc: 0.6310\n",
      "Epoch 11/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.2751 - acc: 0.6382\n",
      "Epoch 00011: val_loss improved from 1.38745 to 1.36028, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 52s 249ms/step - loss: 1.2760 - acc: 0.6381 - val_loss: 1.3603 - val_acc: 0.6381\n",
      "Epoch 12/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.2464 - acc: 0.6442\n",
      "Epoch 00012: val_loss improved from 1.36028 to 1.33874, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 52s 247ms/step - loss: 1.2473 - acc: 0.6440 - val_loss: 1.3387 - val_acc: 0.6429\n",
      "Epoch 13/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.2223 - acc: 0.6483\n",
      "Epoch 00013: val_loss improved from 1.33874 to 1.32076, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 52s 249ms/step - loss: 1.2232 - acc: 0.6482 - val_loss: 1.3208 - val_acc: 0.6455\n",
      "Epoch 14/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.2017 - acc: 0.6517\n",
      "Epoch 00014: val_loss improved from 1.32076 to 1.30602, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 52s 248ms/step - loss: 1.2026 - acc: 0.6515 - val_loss: 1.3060 - val_acc: 0.6485\n",
      "Epoch 15/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.1839 - acc: 0.6549\n",
      "Epoch 00015: val_loss improved from 1.30602 to 1.29342, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 52s 246ms/step - loss: 1.1847 - acc: 0.6547 - val_loss: 1.2934 - val_acc: 0.6513\n",
      "Epoch 16/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.1680 - acc: 0.6579\n",
      "Epoch 00016: val_loss improved from 1.29342 to 1.28291, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 244ms/step - loss: 1.1688 - acc: 0.6577 - val_loss: 1.2829 - val_acc: 0.6550\n",
      "Epoch 17/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.1540 - acc: 0.6602\n",
      "Epoch 00017: val_loss improved from 1.28291 to 1.27385, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 244ms/step - loss: 1.1547 - acc: 0.6600 - val_loss: 1.2739 - val_acc: 0.6562\n",
      "Epoch 18/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.1408 - acc: 0.6624\n",
      "Epoch 00018: val_loss improved from 1.27385 to 1.26614, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 245ms/step - loss: 1.1416 - acc: 0.6622 - val_loss: 1.2661 - val_acc: 0.6580\n",
      "Epoch 19/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.1292 - acc: 0.6651\n",
      "Epoch 00019: val_loss improved from 1.26614 to 1.25936, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 245ms/step - loss: 1.1299 - acc: 0.6648 - val_loss: 1.2594 - val_acc: 0.6590\n",
      "Epoch 20/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.1183 - acc: 0.6667\n",
      "Epoch 00020: val_loss improved from 1.25936 to 1.25308, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 242ms/step - loss: 1.1190 - acc: 0.6665 - val_loss: 1.2531 - val_acc: 0.6603\n",
      "Epoch 21/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.1084 - acc: 0.6684\n",
      "Epoch 00021: val_loss improved from 1.25308 to 1.24794, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 1.1091 - acc: 0.6683 - val_loss: 1.2479 - val_acc: 0.6613\n",
      "Epoch 22/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0989 - acc: 0.6698\n",
      "Epoch 00022: val_loss improved from 1.24794 to 1.24333, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 1.0996 - acc: 0.6696 - val_loss: 1.2433 - val_acc: 0.6619\n",
      "Epoch 23/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0898 - acc: 0.6715\n",
      "Epoch 00023: val_loss improved from 1.24333 to 1.23907, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0905 - acc: 0.6713 - val_loss: 1.2391 - val_acc: 0.6627\n",
      "Epoch 24/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0814 - acc: 0.6726\n",
      "Epoch 00024: val_loss improved from 1.23907 to 1.23499, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0821 - acc: 0.6724 - val_loss: 1.2350 - val_acc: 0.6640\n",
      "Epoch 25/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0736 - acc: 0.6742\n",
      "Epoch 00025: val_loss improved from 1.23499 to 1.23171, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0743 - acc: 0.6741 - val_loss: 1.2317 - val_acc: 0.6642\n",
      "Epoch 26/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0660 - acc: 0.6754\n",
      "Epoch 00026: val_loss improved from 1.23171 to 1.22885, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0667 - acc: 0.6752 - val_loss: 1.2288 - val_acc: 0.6647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0619 - acc: 0.6765\n",
      "Epoch 00027: val_loss improved from 1.22885 to 1.22574, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 54s 256ms/step - loss: 1.0625 - acc: 0.6764 - val_loss: 1.2257 - val_acc: 0.6645\n",
      "Epoch 28/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0530 - acc: 0.6782\n",
      "Epoch 00028: val_loss improved from 1.22574 to 1.22332, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0536 - acc: 0.6781 - val_loss: 1.2233 - val_acc: 0.6656\n",
      "Epoch 29/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0464 - acc: 0.6791\n",
      "Epoch 00029: val_loss improved from 1.22332 to 1.22107, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0471 - acc: 0.6790 - val_loss: 1.2211 - val_acc: 0.6662\n",
      "Epoch 30/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0402 - acc: 0.6798\n",
      "Epoch 00030: val_loss improved from 1.22107 to 1.21909, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0409 - acc: 0.6797 - val_loss: 1.2191 - val_acc: 0.6663\n",
      "Epoch 31/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0344 - acc: 0.6808\n",
      "Epoch 00031: val_loss improved from 1.21909 to 1.21712, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0351 - acc: 0.6807 - val_loss: 1.2171 - val_acc: 0.6666\n",
      "Epoch 32/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0289 - acc: 0.6821\n",
      "Epoch 00032: val_loss improved from 1.21712 to 1.21591, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 1.0296 - acc: 0.6820 - val_loss: 1.2159 - val_acc: 0.6672\n",
      "Epoch 33/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0238 - acc: 0.6830\n",
      "Epoch 00033: val_loss improved from 1.21591 to 1.21416, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 1.0244 - acc: 0.6828 - val_loss: 1.2142 - val_acc: 0.6681\n",
      "Epoch 34/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0186 - acc: 0.6837\n",
      "Epoch 00034: val_loss improved from 1.21416 to 1.21338, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0192 - acc: 0.6835 - val_loss: 1.2134 - val_acc: 0.6684\n",
      "Epoch 35/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0137 - acc: 0.6843\n",
      "Epoch 00035: val_loss improved from 1.21338 to 1.21203, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0143 - acc: 0.6842 - val_loss: 1.2120 - val_acc: 0.6683\n",
      "Epoch 36/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0088 - acc: 0.6851\n",
      "Epoch 00036: val_loss improved from 1.21203 to 1.21113, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 1.0095 - acc: 0.6849 - val_loss: 1.2111 - val_acc: 0.6694\n",
      "Epoch 37/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 1.0042 - acc: 0.6859\n",
      "Epoch 00037: val_loss improved from 1.21113 to 1.21054, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 1.0049 - acc: 0.6857 - val_loss: 1.2105 - val_acc: 0.6694\n",
      "Epoch 38/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9997 - acc: 0.6868\n",
      "Epoch 00038: val_loss improved from 1.21054 to 1.20921, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 244ms/step - loss: 1.0004 - acc: 0.6866 - val_loss: 1.2092 - val_acc: 0.6690\n",
      "Epoch 39/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9953 - acc: 0.6877\n",
      "Epoch 00039: val_loss improved from 1.20921 to 1.20907, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.9960 - acc: 0.6876 - val_loss: 1.2091 - val_acc: 0.6701\n",
      "Epoch 40/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9911 - acc: 0.6883\n",
      "Epoch 00040: val_loss improved from 1.20907 to 1.20782, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9918 - acc: 0.6881 - val_loss: 1.2078 - val_acc: 0.6703\n",
      "Epoch 41/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9870 - acc: 0.6892\n",
      "Epoch 00041: val_loss improved from 1.20782 to 1.20779, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.9877 - acc: 0.6891 - val_loss: 1.2078 - val_acc: 0.6708\n",
      "Epoch 42/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9827 - acc: 0.6902\n",
      "Epoch 00042: val_loss improved from 1.20779 to 1.20721, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9834 - acc: 0.6901 - val_loss: 1.2072 - val_acc: 0.6711\n",
      "Epoch 43/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9791 - acc: 0.6910\n",
      "Epoch 00043: val_loss improved from 1.20721 to 1.20654, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9798 - acc: 0.6908 - val_loss: 1.2065 - val_acc: 0.6712\n",
      "Epoch 44/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9748 - acc: 0.6913\n",
      "Epoch 00044: val_loss did not improve from 1.20654\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9755 - acc: 0.6912 - val_loss: 1.2066 - val_acc: 0.6712\n",
      "Epoch 45/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9715 - acc: 0.6922\n",
      "Epoch 00045: val_loss improved from 1.20654 to 1.20536, saving model to LaaL_cond_hiddim60-.hdf5\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.9722 - acc: 0.6920 - val_loss: 1.2054 - val_acc: 0.6713\n",
      "Epoch 46/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9675 - acc: 0.6929\n",
      "Epoch 00046: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9682 - acc: 0.6928 - val_loss: 1.2066 - val_acc: 0.6716\n",
      "Epoch 47/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9637 - acc: 0.6936\n",
      "Epoch 00047: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.9644 - acc: 0.6935 - val_loss: 1.2065 - val_acc: 0.6724\n",
      "Epoch 48/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9604 - acc: 0.6943\n",
      "Epoch 00048: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 239ms/step - loss: 0.9611 - acc: 0.6941 - val_loss: 1.2066 - val_acc: 0.6721\n",
      "Epoch 49/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9567 - acc: 0.6947\n",
      "Epoch 00049: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9575 - acc: 0.6945 - val_loss: 1.2056 - val_acc: 0.6719\n",
      "Epoch 50/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9537 - acc: 0.6955\n",
      "Epoch 00050: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9544 - acc: 0.6953 - val_loss: 1.2075 - val_acc: 0.6720\n",
      "Epoch 51/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9501 - acc: 0.6959\n",
      "Epoch 00051: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9508 - acc: 0.6957 - val_loss: 1.2065 - val_acc: 0.6720\n",
      "Epoch 52/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9468 - acc: 0.6964\n",
      "Epoch 00052: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.9476 - acc: 0.6962 - val_loss: 1.2072 - val_acc: 0.6721\n",
      "Epoch 53/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9435 - acc: 0.6973\n",
      "Epoch 00053: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 242ms/step - loss: 0.9442 - acc: 0.6971 - val_loss: 1.2071 - val_acc: 0.6723\n",
      "Epoch 54/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9404 - acc: 0.6980\n",
      "Epoch 00054: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9412 - acc: 0.6978 - val_loss: 1.2081 - val_acc: 0.6718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9372 - acc: 0.6985\n",
      "Epoch 00055: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 239ms/step - loss: 0.9380 - acc: 0.6983 - val_loss: 1.2088 - val_acc: 0.6721\n",
      "Epoch 56/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9342 - acc: 0.6994\n",
      "Epoch 00056: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9349 - acc: 0.6992 - val_loss: 1.2089 - val_acc: 0.6727\n",
      "Epoch 57/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.6995\n",
      "Epoch 00057: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 243ms/step - loss: 0.9322 - acc: 0.6993 - val_loss: 1.2099 - val_acc: 0.6735\n",
      "Epoch 58/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9284 - acc: 0.7003\n",
      "Epoch 00058: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.9291 - acc: 0.7002 - val_loss: 1.2101 - val_acc: 0.6729\n",
      "Epoch 59/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9255 - acc: 0.7008\n",
      "Epoch 00059: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 243ms/step - loss: 0.9262 - acc: 0.7007 - val_loss: 1.2115 - val_acc: 0.6726\n",
      "Epoch 60/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9225 - acc: 0.7012\n",
      "Epoch 00060: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9232 - acc: 0.7011 - val_loss: 1.2120 - val_acc: 0.6726\n",
      "Epoch 61/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9197 - acc: 0.7021\n",
      "Epoch 00061: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9204 - acc: 0.7020 - val_loss: 1.2124 - val_acc: 0.6724\n",
      "Epoch 62/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9170 - acc: 0.7023\n",
      "Epoch 00062: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 240ms/step - loss: 0.9177 - acc: 0.7022 - val_loss: 1.2151 - val_acc: 0.6719\n",
      "Epoch 63/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9148 - acc: 0.7029\n",
      "Epoch 00063: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9156 - acc: 0.7028 - val_loss: 1.2135 - val_acc: 0.6711\n",
      "Epoch 64/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9145 - acc: 0.7029\n",
      "Epoch 00064: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9152 - acc: 0.7028 - val_loss: 1.2160 - val_acc: 0.6729\n",
      "Epoch 65/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9096 - acc: 0.7039\n",
      "Epoch 00065: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.9103 - acc: 0.7038 - val_loss: 1.2197 - val_acc: 0.6720\n",
      "Epoch 66/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9067 - acc: 0.7049\n",
      "Epoch 00066: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9074 - acc: 0.7048 - val_loss: 1.2174 - val_acc: 0.6723\n",
      "Epoch 67/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9058 - acc: 0.7046\n",
      "Epoch 00067: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.9067 - acc: 0.7044 - val_loss: 1.2242 - val_acc: 0.6686\n",
      "Epoch 68/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.9045 - acc: 0.7048\n",
      "Epoch 00068: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9052 - acc: 0.7047 - val_loss: 1.2191 - val_acc: 0.6707\n",
      "Epoch 69/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8996 - acc: 0.7058\n",
      "Epoch 00069: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.9004 - acc: 0.7057 - val_loss: 1.2206 - val_acc: 0.6713\n",
      "Epoch 70/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8972 - acc: 0.7066\n",
      "Epoch 00070: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.8979 - acc: 0.7065 - val_loss: 1.2211 - val_acc: 0.6710\n",
      "Epoch 71/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8949 - acc: 0.7067\n",
      "Epoch 00071: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.8957 - acc: 0.7066 - val_loss: 1.2225 - val_acc: 0.6715\n",
      "Epoch 72/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8920 - acc: 0.7076\n",
      "Epoch 00072: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.8927 - acc: 0.7075 - val_loss: 1.2233 - val_acc: 0.6717\n",
      "Epoch 73/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8899 - acc: 0.7080\n",
      "Epoch 00073: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 51s 241ms/step - loss: 0.8907 - acc: 0.7079 - val_loss: 1.2246 - val_acc: 0.6718\n",
      "Epoch 74/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8876 - acc: 0.7085\n",
      "Epoch 00074: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.8883 - acc: 0.7084 - val_loss: 1.2263 - val_acc: 0.6726\n",
      "Epoch 75/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8856 - acc: 0.7092\n",
      "Epoch 00075: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.8863 - acc: 0.7091 - val_loss: 1.2279 - val_acc: 0.6721\n",
      "Epoch 76/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8829 - acc: 0.7094\n",
      "Epoch 00076: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 50s 240ms/step - loss: 0.8837 - acc: 0.7093 - val_loss: 1.2287 - val_acc: 0.6720\n",
      "Epoch 77/100\n",
      "209/210 [============================>.] - ETA: 0s - loss: 0.8808 - acc: 0.7100\n",
      "Epoch 00077: val_loss did not improve from 1.20536\n",
      "210/210 [==============================] - 66s 313ms/step - loss: 0.8815 - acc: 0.7099 - val_loss: 1.2297 - val_acc: 0.6722\n",
      "Epoch 78/100\n",
      " 21/210 [==>...........................] - ETA: 48s - loss: 0.8653 - acc: 0.7029WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-58c2204cdcc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mn_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     verbose=1,)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath=\"LaaL_cond_hiddim60-.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model_cond.fit_generator(train_generator_cond(), \n",
    "                    validation_data=valid_generator_cond(),\n",
    "                    callbacks=callbacks_list,\n",
    "                    steps_per_epoch = n_train, #batch size is inherently 1 via generator\n",
    "                    validation_steps= n_valid,\n",
    "                    epochs=100,\n",
    "                    verbose=1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample trajectory creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "2\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "17\n",
      "12\n",
      "18\n",
      "20\n",
      "18\n",
      "24\n",
      "22\n",
      "20\n",
      "28\n",
      "29\n",
      "31\n",
      "29\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "39\n",
      "40\n",
      "41\n",
      "39\n",
      "44\n",
      "45\n",
      "41\n",
      "47\n",
      "49\n",
      "46\n",
      "51\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "50\n",
      "58\n",
      "59\n",
      "58\n",
      "59\n",
      "57\n",
      "63\n",
      "64\n",
      "61\n",
      "63\n",
      "64\n",
      "60\n",
      "41\n",
      "67\n",
      "71\n",
      "72\n",
      "73\n",
      "67\n",
      "75\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "74\n",
      "82\n",
      "85\n",
      "86\n",
      "84\n",
      "90\n",
      "88\n",
      "90\n",
      "67\n",
      "91\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "91\n",
      "101\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "100\n",
      "109\n",
      "111\n",
      "108\n",
      "114\n",
      "116\n",
      "112\n",
      "120\n",
      "118\n",
      "120\n",
      "91\n",
      "121\n",
      "125\n",
      "126\n",
      "127\n",
      "121\n",
      "129\n",
      "131\n",
      "132\n",
      "133\n",
      "128\n",
      "135\n",
      "134\n",
      "137\n",
      "121\n",
      "139\n",
      "143\n",
      "144\n",
      "145\n",
      "139\n",
      "147\n",
      "149\n",
      "150\n",
      "151\n",
      "146\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "158\n",
      "152\n",
      "158\n",
      "159\n",
      "160\n",
      "139\n",
      "164\n",
      "160\n",
      "164\n",
      "160\n",
      "165\n",
      "167\n",
      "165\n",
      "168\n",
      "169\n",
      "172\n",
      "173\n",
      "175\n",
      "169\n",
      "175\n",
      "177\n",
      "174\n",
      "179\n",
      "181\n",
      "179\n",
      "181\n",
      "178\n",
      "183\n",
      "182\n",
      "185\n",
      "169\n",
      "186\n",
      "188\n",
      "186\n",
      "190\n",
      "192\n",
      "189\n",
      "194\n",
      "196\n",
      "197\n",
      "198\n",
      "193\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "199\n",
      "205\n",
      "186\n",
      "206\n",
      "207\n",
      "208\n",
      "207\n",
      "208\n",
      "206\n",
      "210\n",
      "212\n",
      "209\n",
      "214\n",
      "215\n",
      "216\n",
      "213\n",
      "218\n",
      "217\n",
      "220\n",
      "206\n",
      "221\n",
      "222\n",
      "221\n",
      "224\n",
      "227\n",
      "224\n",
      "226\n",
      "227\n",
      "223\n",
      "229\n",
      "231\n",
      "232\n",
      "233\n",
      "228\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "234\n",
      "241\n",
      "242\n",
      "221\n",
      "245\n",
      "246\n",
      "242\n",
      "250\n",
      "247\n",
      "256\n",
      "258\n",
      "259\n",
      "260\n",
      "251\n",
      "264\n",
      "266\n",
      "255\n",
      "268\n",
      "269\n",
      "261\n",
      "271\n",
      "242\n",
      "272\n",
      "275\n",
      "280\n",
      "272\n",
      "281\n",
      "283\n",
      "281\n",
      "284\n",
      "285\n",
      "288\n",
      "285\n",
      "291\n",
      "289\n",
      "293\n",
      "295\n",
      "292\n",
      "296\n",
      "299\n",
      "301\n",
      "302\n",
      "298\n",
      "304\n",
      "305\n",
      "285\n",
      "307\n",
      "305\n",
      "309\n",
      "311\n",
      "312\n",
      "313\n",
      "308\n",
      "317\n",
      "314\n",
      "319\n",
      "318\n",
      "323\n",
      "326\n",
      "323\n",
      "322\n",
      "328\n",
      "329\n",
      "305\n",
      "331\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "329\n",
      "339\n",
      "337\n",
      "341\n",
      "343\n",
      "344\n",
      "345\n",
      "340\n",
      "347\n",
      "329\n",
      "348\n",
      "353\n",
      "352\n",
      "351\n",
      "354\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "348\n",
      "357\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "355\n",
      "364\n",
      "363\n",
      "366\n",
      "365\n",
      "369\n",
      "348\n",
      "370\n",
      "374\n",
      "370\n",
      "375\n",
      "376\n",
      "378\n",
      "380\n",
      "381\n",
      "382\n",
      "376\n",
      "388\n",
      "383\n",
      "388\n",
      "387\n",
      "390\n",
      "392\n",
      "389\n",
      "394\n",
      "393\n",
      "397\n",
      "395\n",
      "399\n",
      "400\n",
      "376\n",
      "400\n",
      "401\n",
      "400\n",
      "404\n",
      "403\n",
      "405\n",
      "406\n",
      "407\n",
      "406\n",
      "408\n",
      "409\n",
      "408\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "411\n",
      "415\n",
      "414\n",
      "416\n",
      "400\n",
      "417\n",
      "419\n",
      "417\n",
      "420\n",
      "422\n",
      "425\n",
      "417\n",
      "428\n",
      "429\n",
      "430\n",
      "428\n",
      "431\n",
      "432\n",
      "433\n",
      "431\n",
      "434\n",
      "431\n",
      "435\n",
      "434\n",
      "436\n",
      "428\n",
      "437\n",
      "439\n",
      "437\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "442\n",
      "445\n",
      "446\n",
      "447\n",
      "437\n",
      "449\n",
      "453\n",
      "449\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "454\n",
      "464\n",
      "465\n",
      "464\n",
      "465\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "proposed_traj = [0]\n",
    "visit_count = defaultdict(int)\n",
    "max_allowed_visits = 25\n",
    "\n",
    "while len(proposed_traj) < 1000 and (len(proposed_traj) == 1 or proposed_traj[-1] != 0):\n",
    "    x = np.array(proposed_traj).reshape(1,-1)\n",
    "    #run with s = np.ones(x.shpae) for successful\n",
    "    s = np.zeros(x.shape)\n",
    "    for url in reversed(np.argsort(model_cond.predict([x,s])[0,-1])):\n",
    "    #for url in reversed(np.argsort(model_simp.predict(x))):\n",
    "        if visit_count[url] < max_allowed_visits:\n",
    "            proposed_traj.append(url)\n",
    "            visit_count[url] += 1\n",
    "            break\n",
    "    #predicted = np.argmax(model.predict(x)[0,-1])\n",
    "    print(url)\n",
    "    \n",
    "# print(proposed_traj)\n",
    "\n",
    "# cert_traj = pd.DataFrame(proposed_traj)\n",
    "# cert_traj.to_csv('LaaL_simp_traj_hid_dim37.csv', header = ['simple trajectory'], index = False)\n",
    "traj_to_edge_csv(proposed_traj, 'MITxPRO+LASERxB1+1T2019_simulated_unsuccessful_Hdim60.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_traj(sample_traj):\n",
    "    x = np.array(sample_traj[:-1]).reshape(1,-1)\n",
    "    successful = np.ones(x.shape)\n",
    "    unsuccessful = np.zeros(x.shape)\n",
    "    cond_prob_successful = np.array(model_cond([x, successful])).reshape(-1, number_of_URL)\n",
    "    cond_prob_unsuccessful = np.array(model_cond([x, unsuccessful])).reshape(-1, number_of_URL)\n",
    "\n",
    "    score_s = []\n",
    "    score_u = []\n",
    "\n",
    "    for choice, prob_s, prob_u in zip(sample_traj[1:], cond_prob_successful, cond_prob_unsuccessful):\n",
    "        score_s.append(np.log(prob_s[choice]))\n",
    "        score_u.append(np.log(prob_u[choice]))\n",
    "    return score_s, score_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_success(traj):\n",
    "    score_s, score_u = score_traj(traj)\n",
    "    evidence_s = np.log(success_rate) + np.sum(score_s)\n",
    "    evidence_u = np.log(1 - success_rate) + np.sum(score_u)\n",
    "    prob_of_success = 1 / (1 + np.exp(evidence_u - evidence_s))\n",
    "    return prob_of_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(n_train, n_train+n_valid):\n",
    "    user = index[i]\n",
    "    traj = trajectories.iloc[user]\n",
    "    y_true.append(status[user])\n",
    "    y_pred.append(predict_success(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(y_true,y_pred)))\n",
    "pd.DataFrame(confusion_matrix(y_true, np.round(y_pred)))\n",
    "# columns are predictions, rows are true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_s, score_u = score_traj(trajectories.iloc[10])\n",
    "score_s = np.array(score_s)\n",
    "score_u = np.array(score_u)\n",
    "plt.scatter(trajectories.iloc[10][1:], score_s - score_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('what do we call this?')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "\n",
    "# fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "# roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "# # Compute micro-average ROC curve and ROC area\n",
    "# fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "# roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # user list key - session level\n",
    "# AM_userList = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-userList-key-sessionLevel.csv')\n",
    "# AM_userList\n",
    "\n",
    "# # learning pathway network edge lists - edge list for each student in the course that represent a directed \n",
    "# # transitions networks  of students pathway through the courses content modules.  this is all students.\n",
    "# AM_edgelist = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-edges-cohort.csv')\n",
    "# AM_edgelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_traj[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([0,1,3,4,5,6])[0,-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0,1,2,3,4,5,6]).reshape(1,-1)\n",
    "s = np.zeros(x.shape)\n",
    "model2.predict([x,s])[0,-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort([7,5,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendix to the node list that provides a set of XY coordinates to generate a common layout for all networks \n",
    "# produced in the analysis.  force atlas with parameterization <- what is this?\n",
    "AM_node_coord = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-nodes-coordinates-FA2.csv')\n",
    "AM_node_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student identifiers and performance statistics, certification, and enrollment data\n",
    "AM_id_and_performance = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "AM_id_and_performance['certGrp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta data includes the course title, run dates\n",
    "LaaL_meta = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-meta.csv')\n",
    "LaaL_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# complete course structure and module descriptions\n",
    "# list of student identifiers and performance statistics, certification, and enrollment data\n",
    "\n",
    "# LaaL_edgelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-modules.csv')\n",
    "# LaaL_edgelist\n",
    "\n",
    "LaaL_modules = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-modules00.csv')\n",
    "len(LaaL_modules)\n",
    "LaaL_modules[460:470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_edelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-edges.csv')\n",
    "LaaL_edelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_nodelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-nodes.csv')\n",
    "LaaL_nodelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_nodelist = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-nodes-cohort.csv')\n",
    "AM_nodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_node_coord = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-nodes-coordinates-FA2.csv')\n",
    "LaaL_node_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_id_and_performance = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO-LASERxB1-1T2019-auth_user-students.csv')\n",
    "LaaL_id_and_performance[:5]\n",
    "\n",
    "count = LaaL_id_and_performance[LaaL_id_and_performance['certGrp']== 'Certified (< 65% Grade)']\n",
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
