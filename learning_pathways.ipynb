{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, Reshape, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from data_parsing import load_trajectories\n",
    "from functools import partial, reduce\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "# from glob import glob\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories, status = load_trajectories('data/MITxPRO+AMxB+1T2018/edges', 'data/MITxPRO+AMxB+1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "#trajectories = load_trajectories('data/MITxPRO+LASERxB1+1T2019/LaaL')\n",
    "#id_and_performance = pd.read_csv('data/MITxPRO-AMxB-1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "#id_and_performance = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "#id_and_performance.iloc[0]\n",
    "#id_and_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AM_modules = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-modules.txt', sep='\\t', encoding='utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum trajectory identified by number of clicks:  3483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count of students')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "traj_lengths = trajectories.map(len).values\n",
    "plt.hist(traj_lengths)\n",
    "print(\"Maximum trajectory identified by number of clicks: \", max(traj_lengths))\n",
    "plt.title('AM course')\n",
    "plt.xlabel('Learner trajectory length identified by number of clicks')\n",
    "plt.ylabel('Count of students')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7597701149425288"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.arange(len(trajectories))\n",
    "np.random.seed(9)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "incoming_traj = []\n",
    "outgoing_traj = []\n",
    "\n",
    "# this split assumes that we don't need to rebalance for unequal category weights\n",
    "n_valid = int(2*np.sqrt(len(trajectories)))\n",
    "n_train = len(trajectories) - n_valid\n",
    "success_rate = status[index[:n_train]].sum() / n_train\n",
    "\n",
    "for traj in trajectories.values:\n",
    "    incoming_traj.append(np.array(traj[:-1]).reshape(1,-1))\n",
    "    outgoing_traj.append(np.array(traj[1:]).reshape(-1,1))\n",
    "\n",
    "def data_generator(start, stop, use_status):\n",
    "     while True:\n",
    "        for i in range(start, stop):        \n",
    "            x = incoming_traj[index[i]].reshape(1,-1)\n",
    "            s = np.broadcast_to(status[index[i]], x.shape)\n",
    "            y = outgoing_traj[index[i]].reshape(1,-1)\n",
    "            if use_status:\n",
    "                yield [x,s],y\n",
    "            else:\n",
    "                yield x,y\n",
    "\n",
    "train_generator_simp = partial(data_generator, 0, n_train, False)\n",
    "valid_generator_simp = partial(data_generator, n_train, n_train+n_valid, False)\n",
    "train_generator_cond = partial(data_generator, 0, n_train, True)\n",
    "valid_generator_cond = partial(data_generator, n_train, n_train+n_valid, True)\n",
    "success_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "embedding_dim = 30\n",
    "# turning trajectories into sets of URLs\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "number_of_URL = max(trajectories.sum()) + 1\n",
    "#number_of_URL = 1121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "history (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "URL_embedding (Embedding)    (None, None, 30)          22620     \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, None, 30)          7320      \n",
      "_________________________________________________________________\n",
      "Predicted_URL (Dense)        (None, None, 754)         23374     \n",
      "=================================================================\n",
      "Total params: 53,314\n",
      "Trainable params: 53,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ = Input(shape=(None,), name='history')\n",
    "embed = Embedding(number_of_URL, embedding_dim, name='URL_embedding')(input_)\n",
    "\n",
    "rnn = LSTM(hidden_dim, return_sequences=True, name='LSTM')(embed)\n",
    "\n",
    "predicted_URL = Dense(number_of_URL, activation = 'softmax', name='Predicted_URL')(rnn)\n",
    "\n",
    "model_simp = Model(inputs=input_, outputs=predicted_URL, name='Simple_model')\n",
    "model_simp.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "model_simp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conditional_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "history (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "URL_embedding (Embedding)       (None, None, 30)     22620       history[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "status (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM (LSTM)                     (None, None, 30)     7320        URL_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Status_embedding (Embedding)    (None, None, 30)     60          status[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, None, 30)     0           LSTM[0][0]                       \n",
      "                                                                 Status_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Predicted_URL (Dense)           (None, None, 754)    23374       multiply_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 53,374\n",
      "Trainable params: 53,374\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_x = Input(shape=(None,), name='history')\n",
    "input_s = Input(shape=(None,), name='status')\n",
    "embed_x = Embedding(number_of_URL, embedding_dim, name='URL_embedding')(input_x)\n",
    "embed_s = Embedding(2, hidden_dim, embeddings_initializer='ones', name='Status_embedding')(input_s)\n",
    "\n",
    "rnn = LSTM(hidden_dim, return_sequences=True, name='LSTM')(embed_x)\n",
    "masked = Multiply()([rnn, embed_s])\n",
    "\n",
    "predicted_URL = Dense(number_of_URL, activation = 'softmax', name='Predicted_URL')(masked)\n",
    "\n",
    "model_cond = Model(inputs=[input_x, input_s], outputs=predicted_URL, name='Conditional_model')\n",
    "model_cond.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "model_cond.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_simp.load_weights('weights_simp.h5')\n",
    "#model_cond.load_weights('/Users/rsciagli/documents/Y390_dev/weights_improvement/hid_dim30_Lrate_0001_w-improvement-167-1.70.hdf5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 870 steps, validate for 60 steps\n",
      "Epoch 1/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 4.6691 - acc: 0.1281\n",
      "Epoch 00001: val_loss improved from inf to 2.91263, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 360s 414ms/step - loss: 4.6669 - acc: 0.1284 - val_loss: 2.9126 - val_acc: 0.3440\n",
      "Epoch 2/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 2.2844 - acc: 0.4724\n",
      "Epoch 00002: val_loss improved from 2.91263 to 2.22160, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 289s 333ms/step - loss: 2.2836 - acc: 0.4726 - val_loss: 2.2216 - val_acc: 0.4997\n",
      "Epoch 3/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.9276 - acc: 0.5465\n",
      "Epoch 00003: val_loss improved from 2.22160 to 2.07388, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 289s 332ms/step - loss: 1.9270 - acc: 0.5467 - val_loss: 2.0739 - val_acc: 0.5250\n",
      "Epoch 4/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.8193 - acc: 0.5630\n",
      "Epoch 00004: val_loss improved from 2.07388 to 2.01187, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 290s 333ms/step - loss: 1.8187 - acc: 0.5632 - val_loss: 2.0119 - val_acc: 0.5349\n",
      "Epoch 5/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.7651 - acc: 0.5719\n",
      "Epoch 00005: val_loss improved from 2.01187 to 1.97623, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 331ms/step - loss: 1.7645 - acc: 0.5721 - val_loss: 1.9762 - val_acc: 0.5425\n",
      "Epoch 6/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.7308 - acc: 0.5778\n",
      "Epoch 00006: val_loss improved from 1.97623 to 1.95254, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 331ms/step - loss: 1.7302 - acc: 0.5779 - val_loss: 1.9525 - val_acc: 0.5473\n",
      "Epoch 7/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.7065 - acc: 0.5821\n",
      "Epoch 00007: val_loss improved from 1.95254 to 1.93525, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 332ms/step - loss: 1.7059 - acc: 0.5822 - val_loss: 1.9352 - val_acc: 0.5509\n",
      "Epoch 8/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6877 - acc: 0.5857\n",
      "Epoch 00008: val_loss improved from 1.93525 to 1.92171, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 290s 333ms/step - loss: 1.6872 - acc: 0.5858 - val_loss: 1.9217 - val_acc: 0.5539\n",
      "Epoch 9/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6725 - acc: 0.5886\n",
      "Epoch 00009: val_loss improved from 1.92171 to 1.91077, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 331ms/step - loss: 1.6719 - acc: 0.5888 - val_loss: 1.9108 - val_acc: 0.5561\n",
      "Epoch 10/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6597 - acc: 0.5910\n",
      "Epoch 00010: val_loss improved from 1.91077 to 1.90152, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 331ms/step - loss: 1.6591 - acc: 0.5912 - val_loss: 1.9015 - val_acc: 0.5577\n",
      "Epoch 11/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6486 - acc: 0.5931\n",
      "Epoch 00011: val_loss improved from 1.90152 to 1.89338, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 331ms/step - loss: 1.6480 - acc: 0.5933 - val_loss: 1.8934 - val_acc: 0.5598\n",
      "Epoch 12/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6388 - acc: 0.5947\n",
      "Epoch 00012: val_loss improved from 1.89338 to 1.88620, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 331ms/step - loss: 1.6382 - acc: 0.5949 - val_loss: 1.8862 - val_acc: 0.5611\n",
      "Epoch 13/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6301 - acc: 0.5964\n",
      "Epoch 00013: val_loss improved from 1.88620 to 1.87965, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 331ms/step - loss: 1.6295 - acc: 0.5965 - val_loss: 1.8796 - val_acc: 0.5629\n",
      "Epoch 14/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6224 - acc: 0.5977\n",
      "Epoch 00014: val_loss improved from 1.87965 to 1.87390, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 289s 332ms/step - loss: 1.6218 - acc: 0.5979 - val_loss: 1.8739 - val_acc: 0.5642\n",
      "Epoch 15/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6154 - acc: 0.5988\n",
      "Epoch 00015: val_loss improved from 1.87390 to 1.86871, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 292s 336ms/step - loss: 1.6148 - acc: 0.5989 - val_loss: 1.8687 - val_acc: 0.5654\n",
      "Epoch 16/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6091 - acc: 0.6001\n",
      "Epoch 00016: val_loss improved from 1.86871 to 1.86408, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 301s 346ms/step - loss: 1.6086 - acc: 0.6002 - val_loss: 1.8641 - val_acc: 0.5663\n",
      "Epoch 17/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.6034 - acc: 0.6012\n",
      "Epoch 00017: val_loss improved from 1.86408 to 1.85988, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 289s 332ms/step - loss: 1.6028 - acc: 0.6013 - val_loss: 1.8599 - val_acc: 0.5675\n",
      "Epoch 18/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5981 - acc: 0.6020\n",
      "Epoch 00018: val_loss improved from 1.85988 to 1.85607, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 289s 332ms/step - loss: 1.5976 - acc: 0.6022 - val_loss: 1.8561 - val_acc: 0.5683\n",
      "Epoch 19/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5932 - acc: 0.6030\n",
      "Epoch 00019: val_loss improved from 1.85607 to 1.85252, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 291s 334ms/step - loss: 1.5926 - acc: 0.6031 - val_loss: 1.8525 - val_acc: 0.5694\n",
      "Epoch 20/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5886 - acc: 0.6038\n",
      "Epoch 00020: val_loss improved from 1.85252 to 1.84930, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 286s 329ms/step - loss: 1.5881 - acc: 0.6039 - val_loss: 1.8493 - val_acc: 0.5695\n",
      "Epoch 21/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5843 - acc: 0.6045\n",
      "Epoch 00021: val_loss improved from 1.84930 to 1.84636, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 287s 330ms/step - loss: 1.5838 - acc: 0.6047 - val_loss: 1.8464 - val_acc: 0.5703\n",
      "Epoch 22/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5803 - acc: 0.6054\n",
      "Epoch 00022: val_loss improved from 1.84636 to 1.84370, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 288s 331ms/step - loss: 1.5798 - acc: 0.6055 - val_loss: 1.8437 - val_acc: 0.5709\n",
      "Epoch 23/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5766 - acc: 0.6059\n",
      "Epoch 00023: val_loss improved from 1.84370 to 1.84117, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 287s 330ms/step - loss: 1.5760 - acc: 0.6061 - val_loss: 1.8412 - val_acc: 0.5711\n",
      "Epoch 24/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5730 - acc: 0.6065\n",
      "Epoch 00024: val_loss improved from 1.84117 to 1.83887, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 286s 329ms/step - loss: 1.5725 - acc: 0.6067 - val_loss: 1.8389 - val_acc: 0.5720\n",
      "Epoch 25/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5697 - acc: 0.6073\n",
      "Epoch 00025: val_loss improved from 1.83887 to 1.83668, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 287s 329ms/step - loss: 1.5691 - acc: 0.6074 - val_loss: 1.8367 - val_acc: 0.5725\n",
      "Epoch 26/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5665 - acc: 0.6077\n",
      "Epoch 00026: val_loss improved from 1.83668 to 1.83466, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 284s 327ms/step - loss: 1.5659 - acc: 0.6079 - val_loss: 1.8347 - val_acc: 0.5730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5635 - acc: 0.6083\n",
      "Epoch 00027: val_loss improved from 1.83466 to 1.83274, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 311ms/step - loss: 1.5629 - acc: 0.6085 - val_loss: 1.8327 - val_acc: 0.5732\n",
      "Epoch 28/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5606 - acc: 0.6088\n",
      "Epoch 00028: val_loss improved from 1.83274 to 1.83099, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 271s 311ms/step - loss: 1.5601 - acc: 0.6090 - val_loss: 1.8310 - val_acc: 0.5734\n",
      "Epoch 29/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5579 - acc: 0.6093\n",
      "Epoch 00029: val_loss improved from 1.83099 to 1.82933, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 271s 311ms/step - loss: 1.5574 - acc: 0.6095 - val_loss: 1.8293 - val_acc: 0.5738\n",
      "Epoch 30/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5554 - acc: 0.6097\n",
      "Epoch 00030: val_loss improved from 1.82933 to 1.82780, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 311ms/step - loss: 1.5548 - acc: 0.6098 - val_loss: 1.8278 - val_acc: 0.5745\n",
      "Epoch 31/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5529 - acc: 0.6099\n",
      "Epoch 00031: val_loss improved from 1.82780 to 1.82633, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 311ms/step - loss: 1.5523 - acc: 0.6100 - val_loss: 1.8263 - val_acc: 0.5745\n",
      "Epoch 32/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5505 - acc: 0.6102\n",
      "Epoch 00032: val_loss improved from 1.82633 to 1.82494, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 276s 317ms/step - loss: 1.5500 - acc: 0.6104 - val_loss: 1.8249 - val_acc: 0.5751\n",
      "Epoch 33/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5483 - acc: 0.6106\n",
      "Epoch 00033: val_loss improved from 1.82494 to 1.82365, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 278s 320ms/step - loss: 1.5478 - acc: 0.6108 - val_loss: 1.8236 - val_acc: 0.5752\n",
      "Epoch 34/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5461 - acc: 0.6109\n",
      "Epoch 00034: val_loss improved from 1.82365 to 1.82241, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 272s 313ms/step - loss: 1.5456 - acc: 0.6111 - val_loss: 1.8224 - val_acc: 0.5753\n",
      "Epoch 35/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5441 - acc: 0.6113\n",
      "Epoch 00035: val_loss improved from 1.82241 to 1.82127, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5436 - acc: 0.6114 - val_loss: 1.8213 - val_acc: 0.5752\n",
      "Epoch 36/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5421 - acc: 0.6117\n",
      "Epoch 00036: val_loss improved from 1.82127 to 1.82015, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5416 - acc: 0.6118 - val_loss: 1.8202 - val_acc: 0.5756\n",
      "Epoch 37/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5401 - acc: 0.6121\n",
      "Epoch 00037: val_loss improved from 1.82015 to 1.81914, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5396 - acc: 0.6122 - val_loss: 1.8191 - val_acc: 0.5761\n",
      "Epoch 38/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5384 - acc: 0.6124\n",
      "Epoch 00038: val_loss improved from 1.81914 to 1.81816, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 271s 311ms/step - loss: 1.5378 - acc: 0.6125 - val_loss: 1.8182 - val_acc: 0.5765\n",
      "Epoch 39/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5365 - acc: 0.6126\n",
      "Epoch 00039: val_loss improved from 1.81816 to 1.81729, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 272s 312ms/step - loss: 1.5360 - acc: 0.6128 - val_loss: 1.8173 - val_acc: 0.5765\n",
      "Epoch 40/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5347 - acc: 0.6130\n",
      "Epoch 00040: val_loss improved from 1.81729 to 1.81638, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 311ms/step - loss: 1.5342 - acc: 0.6131 - val_loss: 1.8164 - val_acc: 0.5766\n",
      "Epoch 41/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5331 - acc: 0.6132\n",
      "Epoch 00041: val_loss improved from 1.81638 to 1.81559, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5326 - acc: 0.6133 - val_loss: 1.8156 - val_acc: 0.5767\n",
      "Epoch 42/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5315 - acc: 0.6134\n",
      "Epoch 00042: val_loss improved from 1.81559 to 1.81472, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5309 - acc: 0.6135 - val_loss: 1.8147 - val_acc: 0.5771\n",
      "Epoch 43/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5300 - acc: 0.6137\n",
      "Epoch 00043: val_loss improved from 1.81472 to 1.81397, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 271s 312ms/step - loss: 1.5294 - acc: 0.6138 - val_loss: 1.8140 - val_acc: 0.5770\n",
      "Epoch 44/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5284 - acc: 0.6139\n",
      "Epoch 00044: val_loss improved from 1.81397 to 1.81325, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5279 - acc: 0.6141 - val_loss: 1.8133 - val_acc: 0.5773\n",
      "Epoch 45/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5270 - acc: 0.6141\n",
      "Epoch 00045: val_loss improved from 1.81325 to 1.81259, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5265 - acc: 0.6143 - val_loss: 1.8126 - val_acc: 0.5774\n",
      "Epoch 46/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5255 - acc: 0.6142\n",
      "Epoch 00046: val_loss improved from 1.81259 to 1.81183, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5250 - acc: 0.6144 - val_loss: 1.8118 - val_acc: 0.5778\n",
      "Epoch 47/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5242 - acc: 0.6144\n",
      "Epoch 00047: val_loss improved from 1.81183 to 1.81117, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 271s 311ms/step - loss: 1.5237 - acc: 0.6146 - val_loss: 1.8112 - val_acc: 0.5778\n",
      "Epoch 48/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5229 - acc: 0.6147\n",
      "Epoch 00048: val_loss improved from 1.81117 to 1.81056, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 270s 310ms/step - loss: 1.5224 - acc: 0.6149 - val_loss: 1.8106 - val_acc: 0.5776\n",
      "Epoch 49/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5216 - acc: 0.6149\n",
      "Epoch 00049: val_loss improved from 1.81056 to 1.80995, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 271s 312ms/step - loss: 1.5211 - acc: 0.6150 - val_loss: 1.8099 - val_acc: 0.5781\n",
      "Epoch 50/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5204 - acc: 0.6151\n",
      "Epoch 00050: val_loss improved from 1.80995 to 1.80932, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 271s 311ms/step - loss: 1.5199 - acc: 0.6152 - val_loss: 1.8093 - val_acc: 0.5782\n",
      "Epoch 51/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5192 - acc: 0.6152\n",
      "Epoch 00051: val_loss improved from 1.80932 to 1.80882, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 271s 311ms/step - loss: 1.5187 - acc: 0.6154 - val_loss: 1.8088 - val_acc: 0.5786\n",
      "Epoch 52/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5180 - acc: 0.6155\n",
      "Epoch 00052: val_loss improved from 1.80882 to 1.80838, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 274s 314ms/step - loss: 1.5175 - acc: 0.6157 - val_loss: 1.8084 - val_acc: 0.5786\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869/870 [============================>.] - ETA: 0s - loss: 1.5169 - acc: 0.6157\n",
      "Epoch 00053: val_loss improved from 1.80838 to 1.80786, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 292s 335ms/step - loss: 1.5164 - acc: 0.6159 - val_loss: 1.8079 - val_acc: 0.5789\n",
      "Epoch 54/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5158 - acc: 0.6158\n",
      "Epoch 00054: val_loss improved from 1.80786 to 1.80739, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 321s 369ms/step - loss: 1.5152 - acc: 0.6159 - val_loss: 1.8074 - val_acc: 0.5788\n",
      "Epoch 55/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5147 - acc: 0.6160\n",
      "Epoch 00055: val_loss improved from 1.80739 to 1.80696, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 423s 487ms/step - loss: 1.5142 - acc: 0.6161 - val_loss: 1.8070 - val_acc: 0.5791\n",
      "Epoch 56/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5137 - acc: 0.6161\n",
      "Epoch 00056: val_loss improved from 1.80696 to 1.80649, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 382s 439ms/step - loss: 1.5131 - acc: 0.6163 - val_loss: 1.8065 - val_acc: 0.5794\n",
      "Epoch 57/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5127 - acc: 0.6163\n",
      "Epoch 00057: val_loss improved from 1.80649 to 1.80610, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 408s 469ms/step - loss: 1.5121 - acc: 0.6164 - val_loss: 1.8061 - val_acc: 0.5799\n",
      "Epoch 58/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5116 - acc: 0.6165\n",
      "Epoch 00058: val_loss improved from 1.80610 to 1.80571, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 346s 397ms/step - loss: 1.5111 - acc: 0.6166 - val_loss: 1.8057 - val_acc: 0.5800\n",
      "Epoch 59/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5107 - acc: 0.6166\n",
      "Epoch 00059: val_loss improved from 1.80571 to 1.80533, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 294s 338ms/step - loss: 1.5102 - acc: 0.6168 - val_loss: 1.8053 - val_acc: 0.5801\n",
      "Epoch 60/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5097 - acc: 0.6168\n",
      "Epoch 00060: val_loss improved from 1.80533 to 1.80497, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 285s 328ms/step - loss: 1.5092 - acc: 0.6170 - val_loss: 1.8050 - val_acc: 0.5800\n",
      "Epoch 61/500\n",
      "869/870 [============================>.] - ETA: 0s - loss: 1.5088 - acc: 0.6169\n",
      "Epoch 00061: val_loss improved from 1.80497 to 1.80460, saving model to AM_cond_hiddim30-.hdf5\n",
      "870/870 [==============================] - 307s 353ms/step - loss: 1.5083 - acc: 0.6170 - val_loss: 1.8046 - val_acc: 0.5803\n",
      "Epoch 62/500\n",
      "840/870 [===========================>..] - ETA: 9s - loss: 1.5122 - acc: 0.6161 "
     ]
    }
   ],
   "source": [
    "filepath=\"AM_cond_hiddim30-.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model_cond.fit_generator(train_generator_cond(), \n",
    "                    validation_data=valid_generator_cond(),\n",
    "                    callbacks=callbacks_list,\n",
    "                    steps_per_epoch = n_train, #batch size is inherently 1 via generator\n",
    "                    validation_steps= n_valid,\n",
    "                    epochs=500,\n",
    "                    verbose=1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample trajectory creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699\n",
      "699\n",
      "699\n",
      "699\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7fa3b0fd0680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rsciagli/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 537, in __del__\n",
      "    handle=self._handle, deleter=self._deleter)\n",
      "  File \"/Users/rsciagli/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 1139, in delete_iterator\n",
      "    tld.op_callbacks, handle, deleter)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x7fa38eb72950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "699\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c15b6828b671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#run with s = np.ones(x.shpae) for successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvisit_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_allowed_visits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mproposed_traj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "proposed_traj = [0]\n",
    "visit_count = defaultdict(int)\n",
    "max_allowed_visits = 25\n",
    "\n",
    "while len(proposed_traj) < 1000 and (len(proposed_traj) == 1 or proposed_traj[-1] != 0):\n",
    "    x = np.array(proposed_traj).reshape(1,-1)\n",
    "    #run with s = np.ones(x.shpae) for successful\n",
    "    s = np.ones(x.shape)\n",
    "    for url in reversed(np.argsort(model_cond.predict([x,s])[0,-1])):\n",
    "        if visit_count[url] < max_allowed_visits:\n",
    "            proposed_traj.append(url)\n",
    "            visit_count[url] += 1\n",
    "            break\n",
    "    #predicted = np.argmax(model.predict(x)[0,-1])\n",
    "    print(url)\n",
    "    \n",
    "print(proposed_traj)\n",
    "\n",
    "cert_traj = pd.DataFrame(proposed_traj)\n",
    "cert_traj.to_csv('AM_cert_traj_hid_dim30.csv', header = ['certificate trajectory'], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_traj(sample_traj):\n",
    "    x = np.array(sample_traj[:-1]).reshape(1,-1)\n",
    "    successful = np.ones(x.shape)\n",
    "    unsuccessful = np.zeros(x.shape)\n",
    "    cond_prob_successful = np.array(model_cond([x, successful])).reshape(-1, number_of_URL)\n",
    "    cond_prob_unsuccessful = np.array(model_cond([x, unsuccessful])).reshape(-1, number_of_URL)\n",
    "\n",
    "    score_s = []\n",
    "    score_u = []\n",
    "\n",
    "    for choice, prob_s, prob_u in zip(sample_traj[1:], cond_prob_successful, cond_prob_unsuccessful):\n",
    "        score_s.append(np.log(prob_s[choice]))\n",
    "        score_u.append(np.log(prob_u[choice]))\n",
    "    return score_s, score_u\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_success(traj):\n",
    "    score_s, score_u = score_traj(traj)\n",
    "    evidence_s = np.log(success_rate) + np.sum(score_s)\n",
    "    evidence_u = np.log(1 - success_rate) + np.sum(score_u)\n",
    "    prob_of_success = 1 / (1 + np.exp(evidence_u - evidence_s))\n",
    "    return prob_of_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7930744869709381\n",
      "1 0.810186779650193\n",
      "1 0.7941541595398978\n",
      "1 0.8296374822253005\n",
      "1 0.7897698562413029\n",
      "1 0.7990590066513582\n",
      "1 0.797644092243031\n",
      "1 0.8101116783542441\n",
      "1 0.8023316312673971\n",
      "1 0.7855642760177172\n",
      "1 0.8396095541138673\n",
      "0 0.7921514691972364\n",
      "1 0.825456610290981\n",
      "1 0.7929943449643936\n",
      "1 0.8376270903286283\n",
      "0 0.7839146395036175\n",
      "1 0.8283916423422953\n",
      "1 0.813320512112722\n",
      "1 0.7988237028649319\n",
      "1 0.8254566102912432\n",
      "0 0.7860163176236256\n",
      "1 0.7836664027967694\n",
      "1 0.7918699486701074\n",
      "0 0.7805878506977747\n",
      "1 0.7996854760080317\n",
      "1 0.7867135625026914\n",
      "1 0.7967757686942963\n",
      "1 0.821982545567871\n",
      "1 0.7999200242933696\n",
      "1 0.7712287385625282\n",
      "1 0.806556235485551\n",
      "1 0.8009340200089168\n",
      "1 0.8021767070382738\n",
      "1 0.8243281374074243\n",
      "1 0.8146512844891796\n",
      "1 0.8022541805850851\n",
      "1 0.8171449468416626\n",
      "1 0.8017889963058618\n",
      "0 0.7988237028649319\n",
      "1 0.7943935192457867\n",
      "1 0.7940743271130337\n",
      "1 0.7956666195154496\n",
      "0 0.7968548217601484\n",
      "1 0.805181241321482\n",
      "1 0.7896076683366343\n",
      "1 0.7876950865510524\n",
      "0 0.7876134190563315\n",
      "0 0.7878175447779102\n",
      "1 0.7933548033920851\n",
      "1 0.8017889963058618\n",
      "0 0.7994507217024618\n",
      "1 0.835893017493802\n",
      "1 0.7896076683366343\n",
      "0 0.8191789348770616\n",
      "0 0.772454057092778\n",
      "0 0.826229114469237\n",
      "1 0.795030802980847\n",
      "1 0.8081510563830748\n",
      "1 0.8231939313464057\n",
      "1 0.7899319523806072\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_train, n_train+n_valid):\n",
    "    user = index[i]\n",
    "    traj = trajectories.iloc[user]\n",
    "    ground_truth = status[user]\n",
    "    print(ground_truth, predict_success(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # user list key - session level\n",
    "# AM_userList = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-userList-key-sessionLevel.csv')\n",
    "# AM_userList\n",
    "\n",
    "# # learning pathway network edge lists - edge list for each student in the course that represent a directed \n",
    "# # transitions networks  of students pathway through the courses content modules.  this is all students.\n",
    "# AM_edgelist = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-edges-cohort.csv')\n",
    "# AM_edgelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_traj[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([0,1,3,4,5,6])[0,-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0,1,2,3,4,5,6]).reshape(1,-1)\n",
    "s = np.zeros(x.shape)\n",
    "model2.predict([x,s])[0,-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort([7,5,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node list of all students' learning pathway networks\n",
    "AM_nodelist = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-nodes-cohort.csv')\n",
    "AM_nodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendix to the node list that provides a set of XY coordinates to generate a common layout for all networks \n",
    "# produced in the analysis.  force atlas with parameterization <- what is this?\n",
    "AM_node_coord = pd.read_csv('edx-learnerpathway-modeling/data/MITxPRO+AMxB+1T2018/MITxPRO+AMxB+1T2018-stdAgg-nodes-coordinates-FA2.csv')\n",
    "AM_node_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student identifiers and performance statistics, certification, and enrollment data\n",
    "AM_id_and_performance = pd.read_csv('data/MITxPRO+AMxB+1T2018/MITxPRO-AMxB-1T2018-auth_user-students.csv')\n",
    "AM_id_and_performance['certGrp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta data includes the course title, run dates\n",
    "LaaL_meta = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-meta.csv')\n",
    "LaaL_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# complete course structure and module descriptions\n",
    "# list of student identifiers and performance statistics, certification, and enrollment data\n",
    "\n",
    "# LaaL_edgelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-modules.csv')\n",
    "# LaaL_edgelist\n",
    "\n",
    "LaaL_modules = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-modules00.csv')\n",
    "len(LaaL_modules)\n",
    "LaaL_modules[460:470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_edelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-edges.csv')\n",
    "LaaL_edelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_nodelist = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-nodes.csv')\n",
    "LaaL_nodelist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_node_coord = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO+LASERxB1+1T2019-stdAgg-nodes-coordinates-FA2.csv')\n",
    "LaaL_node_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaaL_id_and_performance = pd.read_csv('data/MITxPRO+LASERxB1+1T2019/MITxPRO-LASERxB1-1T2019-auth_user-students.csv')\n",
    "LaaL_id_and_performance[:5]\n",
    "\n",
    "count = LaaL_id_and_performance[LaaL_id_and_performance['certGrp']== 'Certified (< 65% Grade)']\n",
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
